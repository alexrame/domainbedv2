{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import peft\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "\n",
    "class ScriptArguments:\n",
    "    #model_name = \"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment\"\n",
    "    sentiment_models = [\n",
    "        \"lvwerra/distilbert-imdb\", \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"martin-ha/toxic-comment-model\",\n",
    "        \"valurank/distilbert-quality\"\n",
    "    ]\n",
    "    model_names = [\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\",\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\"\n",
    "    ]\n",
    "\n",
    "\n",
    "script_args = ScriptArguments()\n",
    "\n",
    "def load_model(peft_model_id):\n",
    "    peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        return_dict=True,\n",
    "        #torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # Load the Lora model\n",
    "    model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "dict_models_to_merge = {model_name: load_model(model_name) for model_name in script_args.model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PeftConfig.from_pretrained(script_args.model_names[0]).base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\"I really enjoyed the slight hint towards\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:195: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "dict_outputs = {}\n",
    "with torch.cuda.amp.autocast():\n",
    "    for model_name, model in dict_models_to_merge.items():\n",
    "        dict_outputs[model_name] = model.generate(**batch, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05': 'I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-written. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very', 'alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05': 'I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very well-told. The story is a great story and it is a great story'}\n"
     ]
    }
   ],
   "source": [
    "dict_responses = {model_name: tokenizer.decode(output_tokens[0], skip_special_tokens=True) for model_name, output_tokens in dict_outputs.items()}\n",
    "print(dict_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_outputs = OrderedDict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentiment model with ['lvwerra/distilbert-imdb', 'distilbert-base-uncased-finetuned-sst-2-english', 'martin-ha/toxic-comment-model', 'valurank/distilbert-quality']\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\"sentiment-analysis\", model=sentiment_model, device=device)\n",
    "    for sentiment_model in script_args.sentiment_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rewards(rewards):\n",
    "    return [sum(reward)/len(reward) for reward in rewards]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rewards(rewards):\n",
    "    avg_reward = None\n",
    "    for reward in rewards:\n",
    "        if avg_reward is None:\n",
    "            avg_reward = copy.deepcopy(reward)\n",
    "        else:\n",
    "            for a, r in zip(avg_reward, reward):\n",
    "                for i, rr in enumerate(r):\n",
    "                    for j, rrr in enumerate(rr):\n",
    "                        assert a[i][j][\"label\"] == rrr[\"label\"]\n",
    "                        a[i][j][\"score\"] = a[i][j][\"score\"] + rrr[\"score\"]\n",
    "\n",
    "    for a in avg_reward:\n",
    "        for i, r in enumerate(a):\n",
    "            for j, rr in enumerate(r):\n",
    "                rr[\"score\"] = rr[\"score\"] / len(rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = average_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'label': 'NEGATIVE', 'score': -2.6297930081685386},\n",
       "   {'label': 'POSITIVE', 'score': 2.885819753011068}]],\n",
       " [[{'label': 'NEGATIVE', 'score': -4.354640483856201},\n",
       "   {'label': 'POSITIVE', 'score': 4.732121626536052}]],\n",
       " [[{'label': 'non-toxic', 'score': 3.590243419011434},\n",
       "   {'label': 'toxic', 'score': -3.531656821568807}]],\n",
       " [[{'label': 'bad', 'score': 2.913426240285238},\n",
       "   {'label': 'medium', 'score': -3.8566678365071616},\n",
       "   {'label': 'good', 'score': 0.32789580275615055}]]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-written. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.620770215988159}, {'label': 'POSITIVE', 'score': 2.8767237663269043}]], [[{'label': 'NEGATIVE', 'score': -4.3469014167785645}, {'label': 'POSITIVE', 'score': 4.727553367614746}]], [[{'label': 'non-toxic', 'score': 3.601452589035034}, {'label': 'toxic', 'score': -3.5355215072631836}]], [[{'label': 'bad', 'score': 2.964066505432129}, {'label': 'medium', 'score': -3.900128126144409}, {'label': 'good', 'score': 0.32938677072525024}]]]\n",
      "\n",
      "\n",
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very well-told. The story is a great story and it is a great story\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.6475958824157715}, {'label': 'POSITIVE', 'score': 2.9031929969787598}]], [[{'label': 'NEGATIVE', 'score': -4.358898639678955}, {'label': 'POSITIVE', 'score': 4.7377238273620605}]], [[{'label': 'non-toxic', 'score': 3.6023776531219482}, {'label': 'toxic', 'score': -3.5358164310455322}]], [[{'label': 'bad', 'score': 3.3023691177368164}, {'label': 'medium', 'score': -3.8106775283813477}, {'label': 'good', 'score': -0.057702772319316864}]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses_text = list(dict_responses.values())\n",
    "\n",
    "rewards = get_rewards(responses_text)\n",
    "\n",
    "for model_name, text, reward in zip(dict_models_to_merge.keys(), responses_text, rewards):\n",
    "    print(\"model:\", model_name)\n",
    "    print(\"text:\", text)\n",
    "    print(\"reward:\", reward)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  \r"
     ]
    }
   ],
   "source": [
    "def average_weights(input_models, coefficients):\n",
    "    \"\"\"average weights of different transformer models based on the amount of training data they were trained on\"\"\"\n",
    "    weights_averaged = OrderedDict()\n",
    "    for i, current_model in tqdm(enumerate(input_models), leave=False):\n",
    "        current_weights = current_model.state_dict()\n",
    "        for key in current_weights.keys():\n",
    "            if i == 0:\n",
    "                weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "            else:\n",
    "                weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "\n",
    "    return weights_averaged\n",
    "\n",
    "weights_averaged = average_weights(dict_models_to_merge.values(), [1/len(dict_models_to_merge)]*len(dict_models_to_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models_to_merge[\"wa\"] = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05', 'alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05', 'wa'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_models_to_merge.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "dict_outputs = OrderedDict({})\n",
    "with torch.cuda.amp.autocast():\n",
    "    for model_name, model in dict_models_to_merge.items():\n",
    "        dict_outputs[model_name] = model.generate(**batch, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very well-told. The characters are very well-told and the story is very'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_responses[\"wa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rame/ipykernel_3217221/3043154594.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful music. The music is very well-crafted and the story is very well-told. The music is a great story and the story is very well-told. The music is a great story and the story is very well-\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.6299073696136475}, {'label': 'POSITIVE', 'score': 2.8892428874969482}]], [[{'label': 'NEGATIVE', 'score': -4.351489067077637}, {'label': 'POSITIVE', 'score': 4.72714376449585}]], [[{'label': 'non-toxic', 'score': 3.627135992050171}, {'label': 'toxic', 'score': -3.5429253578186035}]], [[{'label': 'bad', 'score': 3.1329898834228516}, {'label': 'medium', 'score': -3.6754684448242188}, {'label': 'good', 'score': 0.00465099373832345}]]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful story. The beautiful and beautiful story is a wonderful and beautiful story. The beautiful and beautiful story is a wonderful and beautiful story. The beautiful and beautiful story is a wonderful and beautiful story. The beautiful and beautiful story is a wonderful\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.6820430755615234}, {'label': 'POSITIVE', 'score': 2.9311652183532715}]], [[{'label': 'NEGATIVE', 'score': -4.358129978179932}, {'label': 'POSITIVE', 'score': 4.720330238342285}]], [[{'label': 'non-toxic', 'score': 3.64192271232605}, {'label': 'toxic', 'score': -3.545462131500244}]], [[{'label': 'bad', 'score': 3.291238784790039}, {'label': 'medium', 'score': -3.6427667140960693}, {'label': 'good', 'score': -0.1873566061258316}]]]\n",
      "\n",
      "\n",
      "model: wa\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful music. The music is very well-crafted and the story is very well-told. The music is a great story and the story is very well-told. The music is a great story and the story is very well-\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.6299073696136475}, {'label': 'POSITIVE', 'score': 2.8892428874969482}]], [[{'label': 'NEGATIVE', 'score': -4.351489067077637}, {'label': 'POSITIVE', 'score': 4.72714376449585}]], [[{'label': 'non-toxic', 'score': 3.627135992050171}, {'label': 'toxic', 'score': -3.5429253578186035}]], [[{'label': 'bad', 'score': 3.1329898834228516}, {'label': 'medium', 'score': -3.6754684448242188}, {'label': 'good', 'score': 0.00465099373832345}]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"I really enjoyed the slight hint towards\", return_tensors=\"pt\")\n",
    "\n",
    "for model_name, model in dict_models_to_merge.items():\n",
    "    responses_text, rewards, reward = get_prediction_rewards(model, batch[\"input_ids\"])\n",
    "    print(\"model:\", model_name)\n",
    "    print(\"text:\", responses_text[0])\n",
    "    print(\"reward:\", rewards[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "    base_model = list(dict_models_to_merge.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_copy = copy.deepcopy(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/rame/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /home/rame/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-7b9524f2f05076d7.arrow\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.                                                                          \n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\n",
      "text: I was very impressed with this film. It is a great story, and a great film. It is a great story, and a great film. It is a great story, and a great story. It is a great story, and a great story.\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.665740489959717}, {'label': 'POSITIVE', 'score': 2.9380452632904053}]], [[{'label': 'NEGATIVE', 'score': -4.344231605529785}, {'label': 'POSITIVE', 'score': 4.735471248626709}]], [[{'label': 'non-toxic', 'score': 3.6214146614074707}, {'label': 'toxic', 'score': -3.541447877883911}]], [[{'label': 'bad', 'score': 2.7662408351898193}, {'label': 'medium', 'score': -3.089421033859253}, {'label': 'good', 'score': -0.29634371399879456}]]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\n",
      "text: I was a great experience. I loved the film and I am so glad it was a great experience. I am so glad I have found this film. It is a great story and a great film. It is a great story and a great film. It\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.615633726119995}, {'label': 'POSITIVE', 'score': 2.879835605621338}]], [[{'label': 'NEGATIVE', 'score': -4.297640323638916}, {'label': 'POSITIVE', 'score': 4.662707805633545}]], [[{'label': 'non-toxic', 'score': 3.56888484954834}, {'label': 'toxic', 'score': -3.5238912105560303}]], [[{'label': 'bad', 'score': 3.2869949340820312}, {'label': 'medium', 'score': -2.9423885345458984}, {'label': 'good', 'score': -0.8983559608459473}]]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: wa\n",
      "text: I was very impressed with this film. It is a great story, and a great film. It is a great story, and a great film. It is a great story, and a great story. It is a great story, and a great story.\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.665740489959717}, {'label': 'POSITIVE', 'score': 2.9380452632904053}]], [[{'label': 'NEGATIVE', 'score': -4.344231605529785}, {'label': 'POSITIVE', 'score': 4.735471248626709}]], [[{'label': 'non-toxic', 'score': 3.6214146614074707}, {'label': 'toxic', 'score': -3.541447877883911}]], [[{'label': 'bad', 'score': 2.7662408351898193}, {'label': 'medium', 'score': -3.089421033859253}, {'label': 'good', 'score': -0.29634371399879456}]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_query_tensors(bs=16):\n",
    "    ds = load_dataset(\"imdb\", split=\"test\")\n",
    "    ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "    input_min_text_length=2\n",
    "    input_max_text_length=8\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    #### get a batch from the dataset\n",
    "    ds.set_format(\"pandas\")\n",
    "    df_batch = ds[:].sample(bs)\n",
    "    query_tensors = df_batch['input_ids'].tolist()\n",
    "    return query_tensors\n",
    "\n",
    "query_tensors = get_query_tensors(bs=16)\n",
    "for model_name, model in dict_models_to_merge.items():\n",
    "    responses_text, rewards, reward = get_prediction_rewards(model, query_tensors)\n",
    "    print(\"model:\", model_name)\n",
    "    print(\"text:\", responses_text[0])\n",
    "    print(\"reward:\", rewards[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very well-told. The characters are very well-told and the story is very\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.620891571044922}, {'label': 'POSITIVE', 'score': 2.8771331310272217}]], [[{'label': 'NEGATIVE', 'score': -4.352511405944824}, {'label': 'POSITIVE', 'score': 4.729320526123047}]], [[{'label': 'non-toxic', 'score': 3.5841763019561768}, {'label': 'toxic', 'score': -3.5295770168304443}]], [[{'label': 'bad', 'score': 2.7189548015594482}, {'label': 'medium', 'score': -3.8796629905700684}, {'label': 'good', 'score': 0.5206950902938843}]]]\n",
      "\n",
      "\n",
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very well-told. The story is a great story and it is a great story\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.6475958824157715}, {'label': 'POSITIVE', 'score': 2.9031929969787598}]], [[{'label': 'NEGATIVE', 'score': -4.358898639678955}, {'label': 'POSITIVE', 'score': 4.7377238273620605}]], [[{'label': 'non-toxic', 'score': 3.6023776531219482}, {'label': 'toxic', 'score': -3.5358164310455322}]], [[{'label': 'bad', 'score': 3.3023691177368164}, {'label': 'medium', 'score': -3.8106775283813477}, {'label': 'good', 'score': -0.057702772319316864}]]]\n",
      "\n",
      "\n",
      "model: wa\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful story. The characters are very well-written and the story is very well-told. The characters are very well-told and the story is very well-told. The characters are very well-told and the story is very\n",
      "reward: [[[{'label': 'NEGATIVE', 'score': -2.620891571044922}, {'label': 'POSITIVE', 'score': 2.8771331310272217}]], [[{'label': 'NEGATIVE', 'score': -4.352511405944824}, {'label': 'POSITIVE', 'score': 4.729320526123047}]], [[{'label': 'non-toxic', 'score': 3.5841763019561768}, {'label': 'toxic', 'score': -3.5295770168304443}]], [[{'label': 'bad', 'score': 2.7189548015594482}, {'label': 'medium', 'score': -3.8796629905700684}, {'label': 'good', 'score': 0.5206950902938843}]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_responses = {\n",
    "    model_name: tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    for model_name, output_tokens in dict_outputs.items()\n",
    "}\n",
    "responses_text = list(dict_responses.values())\n",
    "rewards = get_rewards(responses_text)\n",
    "for model_name, text, reward in zip(dict_models_to_merge.keys(), responses_text, rewards):\n",
    "    print(\"model:\", model_name)\n",
    "    print(\"text:\", text)\n",
    "    print(\"reward:\", reward)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards(responses_text):\n",
    "    sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "\n",
    "    pipe_outputs = [\n",
    "        [sentiment_pipe(response_text, **sent_kwargs) for sentiment_pipe in sentiment_pipes]\n",
    "        for response_text in responses_text]\n",
    "    return pipe_outputs\n",
    "    rewards = [[output[1][\"score\"] for output in outputs] for outputs in pipe_outputs]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rewards(responses_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tensors(bs=16):\n",
    "    ds = load_dataset(\"imdb\", split=\"test\")\n",
    "    ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "    input_min_text_length=2\n",
    "    input_max_text_length=8\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    #### get a batch from the dataset\n",
    "    ds.set_format(\"pandas\")\n",
    "    df_batch = ds[:].sample(bs)\n",
    "    query_tensors = df_batch['input_ids'].tolist()\n",
    "    return query_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/rame/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Map:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                             | 12927/24872 [00:09<00:09, 1263.34 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2228 > 2048). Running this sequence through the model will result in indexing errors\n",
      "                                                                                                                                                                                                                                                            \r"
     ]
    }
   ],
   "source": [
    "query_tensors = get_query_tensors(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "ds.set_format(\"pandas\")\n",
    "df_batch = ds[:].sample(bs)\n",
    "query_tensors = df_batch['input_ids'].tolist()\n",
    "\n",
    "response_tensors = []\n",
    "responses_text = []\n",
    "for i in range(bs):\n",
    "    query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "    output = model.generate(input_ids=query_tensor).squeeze()\n",
    "    response_tensors.append(output)\n",
    "    response = tokenizer.decode(output)\n",
    "    responses_text.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = get_rewards(responses_text)\n",
    "\n",
    "for text, reward in zip(responses_text, rewards):\n",
    "    print(\"text:\", text)\n",
    "    print(\"reward:\", reward)    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
