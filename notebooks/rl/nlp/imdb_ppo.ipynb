{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/rame/trl/examples/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    parser = HfArgumentParser(ScriptArguments)\n",
    "    script_args = parser.parse_args_into_dataclasses()[0]\n",
    "elif False:\n",
    "    script_args = DefaultArgs()\n",
    "else:\n",
    "    script_args = DefaultArgsMerged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Loader.load_base_model(script_args.base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_base_model(base_model_name):\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model_name, load_in_8bit=True, device_map=\"auto\"\n",
    "        )\n",
    "        base_model = prepare_model_for_int8_training(base_model)\n",
    "        return base_model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_peft_model(base_model, peft_name):\n",
    "        \"\"\"### Apply LoRA\n",
    "        Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`.\n",
    "        \"\"\"\n",
    "        if peft_name in [None, \"none\", \"lora\", \"lora0\"]:\n",
    "            os.environ[\"INITLORA\"] = \"0\" if peft_name == \"lora0\" else \"1\"\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.05,\n",
    "                target_modules=None,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "            )\n",
    "            model = get_peft_model(base_model, lora_config)\n",
    "        else:\n",
    "            model = PeftModel.from_pretrained(base_model, peft_name)\n",
    "        modelvaluehead = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "        return modelvaluehead\n",
    "\n",
    "    @staticmethod\n",
    "    def print_trainable_parameters(model):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Loader.load_peft_model(base_model, peft_name=script_args.peft_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = [key for key, _ in model.pretrained_model.base_model.named_modules() if \"lora\" in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretrained_model.base_model.model.model.layers[0].self_attn.q_proj.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.pretrained_model.base_model.model.model.layers[0].self_attn.q_proj.lora_B.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_list:\n",
    "    parent, target, target_name = model.pretrained_model._get_submodules(key)\n",
    "    assert not isinstance(target, peft.tuners.lora.MergedLinear)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        print(\"found\", target)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(target, peft.tuners.lora.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Loader.load_peft_model(base_model, lora_config=script_args.lora_config)\n",
    "base_parameter_names_afterpeft = [n for n, _ in base_model.named_parameters()]\n",
    "parameter_names_afterpeft = [n for n, _ in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_parameter_names_afterpeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names_afterpeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_parameter(\"lm_head.0.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter(\"pretrained_model.base_model.model.lm_head.0.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter(\"v_head.summary.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_parameter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = [n for n, _ in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter(\"v_head.summary.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter(\"v_head.summary.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_layers=None\n",
    "pattern=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = [n for n, _ in model.named_parameters()]\n",
    "ref_model = deepcopy(model)\n",
    "\n",
    "# if no layers are shared, return copy of model\n",
    "if num_shared_layers is None:\n",
    "    for param_name in parameter_names:\n",
    "        param = ref_model.get_parameter(param_name)\n",
    "        param.requires_grad = False\n",
    "    ref_model.eval()\n",
    "else:\n",
    "    # identify layer name pattern\n",
    "    if pattern is not None:\n",
    "        pattern = pattern.format(layer=num_shared_layers)\n",
    "    else:\n",
    "        for pattern_candidate in LAYER_PATTERNS:\n",
    "            pattern_candidate = pattern_candidate.format(layer=num_shared_layers)\n",
    "            if any([pattern_candidate in name for name in parameter_names]):\n",
    "                pattern = pattern_candidate\n",
    "                break\n",
    "\n",
    "    if pattern is None:\n",
    "        raise ValueError(\"Layer pattern could not be matched.\")\n",
    "\n",
    "    # divide parameters in shared and unshared parameter lists\n",
    "    shared_param_list = []\n",
    "    unshared_param_list = []\n",
    "\n",
    "    shared_parameter = True\n",
    "    for name, param in model.named_parameters():\n",
    "        if pattern in name:\n",
    "            shared_parameter = False\n",
    "        if shared_parameter:\n",
    "            shared_param_list.append(name)\n",
    "        else:\n",
    "            unshared_param_list.append(name)\n",
    "\n",
    "    # create reference of the original parameter if they are shared\n",
    "    for param_name in shared_param_list:\n",
    "        param = model.get_parameter(param_name)\n",
    "        param.requires_grad = False\n",
    "\n",
    "        ref_param = ref_model.get_parameter(param_name)  # noqa\n",
    "        ref_param = param  # noqa\n",
    "\n",
    "    # for all other parameters just make sure they don't use gradients\n",
    "    for param_name in unshared_param_list:\n",
    "        param = ref_model.get_parameter(param_name)\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model.get_parameter(param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DIR\"] = os.path.join(FOLDER_EXPE, \"wandb\")\n",
    "script_args_name = Naming.get_name(script_args)\n",
    "os.environ[\"WANDB_NAME\"] = script_args_name\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=script_args.model_name,\n",
    "    init_kl_coef=script_args.init_kl_coef,\n",
    "    adap_kl_ctrl=script_args.adap_kl_ctrl,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with if script_args.log_with != \"\" else None,\n",
    "    batch_size=script_args.batch_size,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    optimize_cuda_cache=True,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load_tokenizer(script_args.model_name)\n",
    "Loader.print_trainable_parameters(model)\n",
    "\n",
    "instructions = llama_utils.Instructions(prompt=script_args.prompt, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(config, tokenizer=tokenizer)\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate\n",
    ")\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Runner():\n",
    "\n",
    "    def __init__(self, ppo_trainer, sentiment_model, device, output_max_length):\n",
    "        self.ppo_trainer = ppo_trainer\n",
    "        self.generation_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": -1,\n",
    "        }\n",
    "\n",
    "        output_min_length = output_max_length//2\n",
    "        self.output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "        print(f\"Load sentiment model with {sentiment_model}\")\n",
    "        self.sentiment_pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=sentiment_model,\n",
    "            device=device,\n",
    "            tokenizer=llama_utils.load_tokenizer_pipe(sentiment_model)\n",
    "        )\n",
    "        # We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "        # We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "        self.sent_kwargs = {\n",
    "            \"return_all_scores\": True,\n",
    "            \"function_to_apply\": \"none\",\n",
    "            \"batch_size\": config.mini_batch_size\n",
    "        }\n",
    "\n",
    "    def apply_sentiment_pipe(self, texts):\n",
    "        texts = [\n",
    "            llama_utils.transform_text(\n",
    "                sentiment_pipe=self.sentiment_pipe,\n",
    "                response_text=text,\n",
    "                instruction=instructions.instruction_movie\n",
    "            ) for text in texts\n",
    "        ]\n",
    "        pipe_outputs = self.sentiment_pipe(texts, **self.sent_kwargs)\n",
    "\n",
    "        def get_score_from_output(output, score_index):\n",
    "            if score_index == \"\":\n",
    "                return 0.\n",
    "            if score_index in [\"positive\", \"negative\"]:\n",
    "                score_index = {\"positive\": 1, \"negative\": 0}[score_index]\n",
    "            elif \"-\" in score_index:\n",
    "                return get_score_from_output(output,\n",
    "                                            score_index.split(\"-\")[0]) - get_score_from_output(\n",
    "                                                output,\n",
    "                                                score_index.split(\"-\")[1]\n",
    "                                            )\n",
    "            return output[int(score_index)][\"score\"]\n",
    "\n",
    "        rewards = [get_score_from_output(output, script_args.score_goal) for output in pipe_outputs]\n",
    "        return rewards\n",
    "\n",
    "    def train_ppo(self, model):\n",
    "        for epoch, batch in tqdm(enumerate(self.ppo_trainer.dataloader)):\n",
    "            query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "            model.gradient_checkpointing_disable()\n",
    "            model.pretrained_model.config.use_cache = True\n",
    "\n",
    "            # Get response from Causal LM\n",
    "            response_tensors = []\n",
    "            for query in query_tensors:\n",
    "                gen_len = self.output_length_sampler()\n",
    "                self.generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "                response = self.ppo_trainer.generate(query, **self.generation_kwargs)\n",
    "                response_tensors.append(response.squeeze()[-gen_len:])\n",
    "            batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "            # Compute sentiment score\n",
    "            texts = [\". \".join(q.split(\"Response: \")[1:]) + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "            rewards = self.apply_sentiment_pipe(texts)\n",
    "            rewards = [torch.tensor(reward) for reward in rewards]\n",
    "\n",
    "            # Run PPO step\n",
    "            model.gradient_checkpointing_enable()\n",
    "            model.pretrained_model.config.use_cache = False\n",
    "\n",
    "            stats = self.ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "            self.ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "    ppo_trainer,\n",
    "    sentiment_model=script_args.sentiment_model,\n",
    "    device=device,\n",
    "    output_max_length=script_args.output_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train_ppo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
