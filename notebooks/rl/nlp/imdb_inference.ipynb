{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/rame/trl/examples/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from llama_inference_imdb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load LMs with [<llama_experiments.Experiments object at 0x7f2ec9b0edf0>]\n",
      "Load sentiment model with ['OpenAssistant/reward-model-deberta-v3-base', 'ChaiML/gpt2_base_retry_and_continue_5m_reward_model', 'ChaiML/gpt2_base_retry_and_continue_12m_reward_model', 'sugam11/gpt2-rlhf-reward', 'theblackcat102/electra-large-reward-model']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 921/921 [00:00<00:00, 151kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.34G/1.34G [00:33<00:00, 39.9MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 368/368 [00:00<00:00, 36.7kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 993kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 711k/711k [00:00<00:00, 1.87MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 16.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "script_args = DefaultArgs()\n",
    "assert \"llama\" in script_args.base_model_name.lower()\n",
    "print(f\"Load LMs with {script_args.peft_names}\")\n",
    "instructions = Instructions(prompt=script_args.prompt)\n",
    "\n",
    "sentiment_pipes = Pipelines.load_pipes(script_args.sentiment_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions.instruction_movie = \"Generate a movie review.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_text = [\"Isaac Florentine has made some of the most entertaining and action-packed movies of the last decade. His latest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theblackcat102/electra-large-reward-model'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipes[-1].model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "\n",
    "rewards = [\n",
    "    [\n",
    "            llama_utils.transform_text(\n",
    "                sentiment_pipe, response_text, instruction=instructions.instruction_movie\n",
    "            ) for sentiment_pipe in sentiment_pipes\n",
    "    ] for response_text in responses_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]Generate a movie review.[SEP]Isaac Florentine has made some of the most entertaining and action-packed movies of the last decade. His latest',\n",
       "  'User: Generate a movie review.\\nBot: Isaac Florentine has made some of the most entertaining and action-packed movies of the last decade. His latest',\n",
       "  'User: Generate a movie review.\\nBot: Isaac Florentine has made some of the most entertaining and action-packed movies of the last decade. His latest',\n",
       "  'Instruction: Generate a movie review. Response: Isaac Florentine has made some of the most entertaining and action-packed movies of the last decade. His latest',\n",
       "  'Instruction: Generate a movie review. Response: Isaac Florentine has made some of the most entertaining and action-packed movies of the last decade. His latest']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "\n",
    "rewards = [\n",
    "    [\n",
    "        sentiment_pipe(\n",
    "            llama_utils.transform_text(\n",
    "                sentiment_pipe, response_text, instruction=instructions.instruction_movie\n",
    "            ), **sent_kwargs\n",
    "        ) for sentiment_pipe in sentiment_pipes\n",
    "    ] for response_text in responses_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[{'label': 'REWARD', 'score': -2.8863346576690674}]],\n",
       "  [[{'label': 'LABEL_0', 'score': -1.265516996383667},\n",
       "    {'label': 'LABEL_1', 'score': 0.5113178491592407}]],\n",
       "  [[{'label': 'LABEL_0', 'score': -0.8205734491348267},\n",
       "    {'label': 'LABEL_1', 'score': 0.5476537942886353}]],\n",
       "  [[{'label': 'LABEL_0', 'score': -0.1741766482591629}]],\n",
       "  [[{'label': 'LABEL_0', 'score': -2.5120272636413574}]]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [\n",
    "    [\n",
    "        apply_sentiment_pipe(sentiment_pipe, response_text)\n",
    "        for sentiment_pipe in sentiment_pipes\n",
    "    ]\n",
    "    for response_text in responses_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load all key components\n",
    "base_model = Loader.load_base_model(script_args.base_model_name)\n",
    "tokenizer = Tokenizer.load_tokenizer(script_args.base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    parser = HfArgumentParser(ScriptArguments)\n",
    "    script_args = parser.parse_args_into_dataclasses()[0]\n",
    "else:\n",
    "    script_args = NotebookArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Loader.load_base_model(script_args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, lora_model = Loader.load_peft_model(base_model, lora_config=\"nolora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.is_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args = ScriptArguments.notebook_get_args()\n",
    "assert \"llama\" in script_args.base_model_name.lower()\n",
    "print(f\"Load LMs with {script_args.peft_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Loader.load_base_model(script_args.base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = base_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = base_model_v2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[\"model.layers.2.self_attn.rotary_emb.inv_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_v2 = LlamaForCausalLM.from_pretrained(\n",
    "            script_args.peft_names[0],     return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Load LMs with {script_args.model_names}\")\n",
    "dict_models_to_merge = OrderedDict({model_name: load_model(model_name) for model_name in script_args.model_names})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PeftConfig.from_pretrained(script_args.model_names[0]).base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\"sentiment-analysis\", model=sentiment_model, device=device)\n",
    "    for sentiment_model in script_args.sentiment_models]\n",
    "\n",
    "list_states_dict = []\n",
    "for current_model in dict_models_to_merge.values():\n",
    "    current_weights = copy.deepcopy(current_model.state_dict())\n",
    "    list_states_dict.append(current_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_query_tensors = get_samples_query_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.3, 0.7])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.25, 0.75])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for coeff in [x / 20 for x in range(17, -1, -1)]:\n",
    "    wa = enrich_wa_states(list_states_dict, coefficients=[1 - coeff, coeff])\n",
    "    list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors)\n",
    "    print(coeff)\n",
    "    print(list_rewards_wa_samples)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
