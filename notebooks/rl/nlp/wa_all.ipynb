{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "import copy\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "from trl.core import LengthSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptArguments:\n",
    "    sentiment_models = [\n",
    "        \"lvwerra/distilbert-imdb\", \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"martin-ha/toxic-comment-model\", \"valurank/distilbert-quality\"\n",
    "    ]\n",
    "    model_names = [\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model distilbert-base-uncased-finetuned-sst-2-english\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-neg-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model distilbert-base-uncased-finetuned-sst-2-english --score_goal negative\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-toxic-neg-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model martin-ha/toxic-comment-model --score_goal 1\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-toxic-0\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model martin-ha/toxic-comment-model --score_goal 0,\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-1\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model valurank/distilbert-quality --score_goal 1\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2\"\n",
    "        #  cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model valurank/distilbert-quality --score_goal 2\n",
    "    ]\n",
    "    num_samples = 160\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Inference')\n",
    "    parser.add_argument('--sentiment_models', type=str, nargs='+', default=ScriptArguments.sentiment_models)\n",
    "    parser.add_argument('--model_names', type=str, nargs='+', default=ScriptArguments.model_names)\n",
    "    parser.add_argument('--num_samples', type=int, default=ScriptArguments.num_samples)\n",
    "    return parser.parse_args()\n",
    "\n",
    "def notebook_get_args():\n",
    "    return ScriptArguments()\n",
    "\n",
    "script_args = notebook_get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScriptArguments:\n",
    "    sentiment_models = [\n",
    "        \"lvwerra/distilbert-imdb\", \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"martin-ha/toxic-comment-model\", \"valurank/distilbert-quality\"\n",
    "    ]\n",
    "    model_names = [\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\",\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model distilbert-base-uncased-finetuned-sst-2-english\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-neg-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model distilbert-base-uncased-finetuned-sst-2-english --score_goal negative\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-toxic-neg-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model martin-ha/toxic-comment-model --score_goal 1\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2\"\n",
    "    ]\n",
    "\n",
    "script_args = ScriptArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(peft_model_id):\n",
    "    peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        return_dict=True,\n",
    "        #torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # Load the Lora model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        peft_model_id,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def average_weights(input_models, coefficients):\n",
    "    \"\"\"average weights of different transformer models based on the amount of training data they were trained on\"\"\"\n",
    "    weights_averaged = OrderedDict()\n",
    "    for i, current_model in tqdm(enumerate(input_models), leave=False):\n",
    "        current_weights = current_model.state_dict()\n",
    "        for key in current_weights.keys():\n",
    "            if i == 0:\n",
    "                weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "            else:\n",
    "                weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "\n",
    "    return weights_averaged\n",
    "\n",
    "def enrich_wa(dict_models_to_merge, coefficients=None):\n",
    "    if coefficients is None:\n",
    "        coefficients = [1 / len(dict_models_to_merge) for _ in len(dict_models_to_merge)]\n",
    "    weights_averaged = average_weights(dict_models_to_merge.values(), coefficients)\n",
    "    base_model_copy = list(dict_models_to_merge.values())[0]\n",
    "    base_model_copy.load_state_dict(weights_averaged, strict=True)\n",
    "    return base_model_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_samples_query_tensors():\n",
    "    list_texts = [\n",
    "        \"I really enjoyed the slight hint towards\",\n",
    "        \"I really hated the horrible hint towards\"\n",
    "    ]\n",
    "\n",
    "    batch = tokenizer(list_texts, return_tensors=\"pt\")\n",
    "    return batch[\"input_ids\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(dict_models_to_merge, query_tensors, verbose=False):\n",
    "    list_rewards = []\n",
    "    for model_name, model in dict_models_to_merge.items():\n",
    "        responses_text, rewards, avg_reward = get_prediction_rewards(model, query_tensors)\n",
    "        if verbose:\n",
    "            print(\"model:\", model_name)\n",
    "            print(\"avg reward:\", avg_reward)\n",
    "            for text, reward in zip(responses_text, rewards):\n",
    "                print(\"text:\", text)\n",
    "                print(\"reward:\", reward)\n",
    "            print(\"\\n\")\n",
    "        list_rewards.append(avg_reward)\n",
    "    return list_rewards\n",
    "\n",
    "\n",
    "\n",
    "def get_imdb_query_tensors(bs=16):\n",
    "    ds = load_dataset(\"imdb\", split=\"test\")\n",
    "    ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "    input_min_text_length=2\n",
    "    input_max_text_length=8\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    #### get a batch from the dataset\n",
    "    ds.set_format(\"pandas\")\n",
    "    df_batch = ds[:].sample(bs)\n",
    "    query_tensors = df_batch['input_ids'].tolist()\n",
    "    return query_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load LMs with ['alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-lr1.41e-05', 'alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentiment model with ['lvwerra/distilbert-imdb', 'distilbert-base-uncased-finetuned-sst-2-english', 'martin-ha/toxic-comment-model', 'valurank/distilbert-quality']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/rame/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /home/rame/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-771f297e2f2a4a0e.arrow\n",
      "Loading cached processed dataset at /home/rame/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-52416bdbc085e52d.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Load LMs with {script_args.model_names}\")\n",
    "dict_models_to_merge = OrderedDict({model_name: load_model(model_name) for model_name in script_args.model_names})\n",
    "# average\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PeftConfig.from_pretrained(script_args.model_names[0]).base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\"sentiment-analysis\", model=sentiment_model, device=device)\n",
    "    for sentiment_model in script_args.sentiment_models]\n",
    "\n",
    "samples_query_tensors = get_samples_query_tensors()\n",
    "imdb_query_tensors = get_imdb_query_tensors(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_rewards(model, query_tensors):\n",
    "    def get_rewards(responses_text):\n",
    "        sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "        rewards = [\n",
    "            [sentiment_pipe(response_text, **sent_kwargs) for sentiment_pipe in sentiment_pipes]\n",
    "            for response_text in responses_text]\n",
    "\n",
    "        rewards = [transform_reward(reward) for reward in rewards]\n",
    "        return rewards\n",
    "    def transform_reward(reward):\n",
    "        d_reward = []\n",
    "        for rew in reward:\n",
    "            d = {}\n",
    "            assert len(rew) == 1\n",
    "            for r in rew[0]:\n",
    "                d[r[\"label\"]] = r[\"score\"]\n",
    "            d_reward.append(d)\n",
    "        return d_reward\n",
    "\n",
    "    def average_rewards(rewards):\n",
    "        avg_reward = None\n",
    "        for reward in rewards:\n",
    "            if avg_reward is None:\n",
    "                avg_reward = copy.deepcopy(reward)\n",
    "            else:\n",
    "                for a_dict_reward, r_dict_reward in zip(avg_reward, reward):\n",
    "                    for label in a_dict_reward:\n",
    "                        a_dict_reward[label] = a_dict_reward[label] + r_dict_reward[label]\n",
    "\n",
    "        for a_dict_reward in avg_reward:\n",
    "            for label in a_dict_reward:\n",
    "                a_dict_reward[label] = a_dict_reward[label] / len(rewards)\n",
    "        return avg_reward\n",
    "\n",
    "    response_tensors = []\n",
    "    responses_text = []\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    for i in range(len(query_tensors)):\n",
    "        query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "        output = model.generate(input_ids=query_tensor, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id).squeeze()\n",
    "        response_tensors.append(output)\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        responses_text.append(response)\n",
    "\n",
    "    rewards = get_rewards(responses_text)\n",
    "    avg_reward = average_rewards(rewards)\n",
    "    return responses_text, rewards, avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rame/ipykernel_3351357/988401177.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:195: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-lr1.41e-05\n",
      "avg reward: [{'NEGATIVE': -2.3680447340011597, 'POSITIVE': 2.6450458765029907}, {'NEGATIVE': -4.182225227355957, 'POSITIVE': 4.52437949180603}, {'non-toxic': 3.4182801246643066, 'toxic': -3.454858183860779}, {'bad': 3.0607573986053467, 'medium': -2.8268789052963257, 'good': -0.7550765573978424}]\n",
      "text: I really enjoyed the slight hint towards the wonderful and beautiful. The beautiful and beautiful is a wonderful and beautiful. I really enjoyed the beautiful and beautiful. I really enjoyed the beautiful and beautiful. I really enjoyed the beautiful and beautiful. I really enjoyed the beautiful and beautiful. I really enjoyed\n",
      "reward: [{'NEGATIVE': -2.618964195251465, 'POSITIVE': 2.881850004196167}, {'NEGATIVE': -4.359358310699463, 'POSITIVE': 4.702208042144775}, {'non-toxic': 3.474475383758545, 'toxic': -3.48274564743042}, {'bad': 3.0476889610290527, 'medium': -3.3974971771240234, 'good': -0.23794680833816528}]\n",
      "text: I really hated the horrible hint towards the end. I loved the movie and I loved the acting. I loved the story and the acting. I loved the story. I loved the story. I loved the story. I loved the story. I loved the story. I loved the story\n",
      "reward: [{'NEGATIVE': -2.1171252727508545, 'POSITIVE': 2.4082417488098145}, {'NEGATIVE': -4.005092144012451, 'POSITIVE': 4.346550941467285}, {'non-toxic': 3.3620848655700684, 'toxic': -3.4269707202911377}, {'bad': 3.0738258361816406, 'medium': -2.256260633468628, 'good': -1.2722063064575195}]\n",
      "\n",
      "\n",
      "model: alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2\n",
      "avg reward: [{'NEGATIVE': -0.0041397809982299805, 'POSITIVE': -0.06043267250061035}, {'NEGATIVE': 0.2744317054748535, 'POSITIVE': 0.2846871614456177}, {'non-toxic': 3.005249261856079, 'toxic': -3.1387776136398315}, {'bad': 2.2686551213264465, 'medium': -3.126933217048645, 'good': 0.2602297365665436}]\n",
      "text: I really enjoyed the slight hint towards the end of the film. The film was a very interesting and interesting study of the relationship between the two subjects. The film was also very interesting in the way in which the subjects were related to the subject of the film. The film was also very\n",
      "reward: [{'NEGATIVE': -2.424826145172119, 'POSITIVE': 2.707240581512451}, {'NEGATIVE': -4.072668075561523, 'POSITIVE': 4.35998010635376}, {'non-toxic': 3.634641170501709, 'toxic': -3.544497013092041}, {'bad': 1.9676600694656372, 'medium': -3.885385274887085, 'good': 1.1834769248962402}]\n",
      "text: I really hated the horrible hint towards the end of the film. The film was a very poor film, with a very poor score, and the film was very poor in the direction of the film. The film was very poor in the direction of the film, and the film was very\n",
      "reward: [{'NEGATIVE': 2.416546583175659, 'POSITIVE': -2.828105926513672}, {'NEGATIVE': 4.6215314865112305, 'POSITIVE': -3.7906057834625244}, {'non-toxic': 2.375857353210449, 'toxic': -2.733058214187622}, {'bad': 2.569650173187256, 'medium': -2.368481159210205, 'good': -0.6630174517631531}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_rewards_samples = predict(dict_models_to_merge, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'NEGATIVE': -2.3680447340011597, 'POSITIVE': 2.6450458765029907}, {'NEGATIVE': -4.182225227355957, 'POSITIVE': 4.52437949180603}, {'non-toxic': 3.4182801246643066, 'toxic': -3.454858183860779}, {'bad': 3.0607573986053467, 'medium': -2.8268789052963257, 'good': -0.7550765573978424}]\n",
      "[{'NEGATIVE': -0.0041397809982299805, 'POSITIVE': -0.06043267250061035}, {'NEGATIVE': 0.2744317054748535, 'POSITIVE': 0.2846871614456177}, {'non-toxic': 3.005249261856079, 'toxic': -3.1387776136398315}, {'bad': 2.2686551213264465, 'medium': -3.126933217048645, 'good': 0.2602297365665436}]\n"
     ]
    }
   ],
   "source": [
    "for rewards_samples in list_rewards_samples:\n",
    "    print(rewards_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rewards_imdb = predict({\"qual\": dict_models_to_merge[\"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2\"]}, imdb_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rewards_imdb in list_rewards_imdb:\n",
    "    print(rewards_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_states_dict = []\n",
    "for current_model in dict_models_to_merge.values():\n",
    "    current_weights = copy.deepcopy(current_model.state_dict())\n",
    "    list_states_dict.append(current_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1709, -0.7383,  0.4277,  ...,  0.0840,  0.5820, -0.3457],\n",
       "        [ 0.2070, -0.6055,  0.4590,  ...,  0.1562,  0.4883, -0.2363],\n",
       "        [ 0.2324, -0.6367,  0.3262,  ...,  0.2236,  0.7500, -0.2354],\n",
       "        ...,\n",
       "        [ 0.7734, -1.1406,  0.6523,  ...,  0.2832,  0.9258, -0.5547],\n",
       "        [ 0.3906, -0.8438,  0.5117,  ...,  0.0148,  0.6992, -0.2383],\n",
       "        [ 0.2734, -0.7148,  0.2949,  ...,  0.1748,  0.4043, -0.3105]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_states_dict[0]['base_model.model.transformer.wte.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_states_dict(list_states_dict, coefficients):\n",
    "    \"\"\"average weights of different transformer models based on the amount of training data they were trained on\"\"\"\n",
    "    weights_averaged = OrderedDict()\n",
    "    for i, current_weights in enumerate(list_states_dict):\n",
    "        for key in current_weights.keys():\n",
    "            if i == 0:\n",
    "                weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "            else:\n",
    "                weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "    return weights_averaged\n",
    "\n",
    "def enrich_wa_states(list_states_dict, coefficients=None):\n",
    "    weights_averaged = average_states_dict(list_states_dict, coefficients)\n",
    "    base_model_copy = list(dict_models_to_merge.values())[0]\n",
    "    base_model_copy.load_state_dict(weights_averaged, strict=True)\n",
    "    return base_model_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rame/ipykernel_3351357/988401177.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: wa\n",
      "avg reward: [{'NEGATIVE': -2.4099957942962646, 'POSITIVE': 2.6818289756774902}, {'NEGATIVE': -4.221608638763428, 'POSITIVE': 4.549301624298096}, {'non-toxic': 3.129125714302063, 'toxic': -3.239298462867737}, {'bad': 2.641398787498474, 'medium': -2.841304361820221, 'good': -0.3426131308078766}]\n",
      "text: I really enjoyed the slight hint towards the end of the film. The film was a very interesting and interesting film. The film was very interesting and interesting. The film was very interesting and interesting. The film was very interesting and interesting. The film was very interesting and interesting. The film\n",
      "reward: [{'NEGATIVE': -2.3839497566223145, 'POSITIVE': 2.6594905853271484}, {'NEGATIVE': -4.217746257781982, 'POSITIVE': 4.57012414932251}, {'non-toxic': 3.6582894325256348, 'toxic': -3.54632830619812}, {'bad': 2.9145758152008057, 'medium': -4.4980363845825195, 'good': 0.8217939734458923}]\n",
      "text: I really hated the horrible hint towards the end of the film. The film was a bit of a disappointment, but the film was a good and enjoyable one. The film was a good film, and I enjoyed it. The film was a good film, and I enjoyed it. The\n",
      "reward: [{'NEGATIVE': -2.436041831970215, 'POSITIVE': 2.704167366027832}, {'NEGATIVE': -4.225471019744873, 'POSITIVE': 4.528479099273682}, {'non-toxic': 2.599961996078491, 'toxic': -2.9322686195373535}, {'bad': 2.3682217597961426, 'medium': -1.1845723390579224, 'good': -1.5070202350616455}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rame/ipykernel_3351357/988401177.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: wa\n",
      "avg reward: [{'NEGATIVE': -2.1424390077590942, 'POSITIVE': 2.373133957386017}, {'NEGATIVE': -4.016600131988525, 'POSITIVE': 4.373375415802002}, {'non-toxic': 3.3106372356414795, 'toxic': -3.381856083869934}, {'bad': 2.9735552072525024, 'medium': -3.3110193014144897, 'good': -0.21162253618240356}]\n",
      "text: I really enjoyed the slight hint towards the end of the film. The film was very well done and the film was very well presented. The film was well presented and the film was well presented. The film was well presented and the film was well presented. The film was well presented.\n",
      "reward: [{'NEGATIVE': -2.4986557960510254, 'POSITIVE': 2.762079954147339}, {'NEGATIVE': -4.267558574676514, 'POSITIVE': 4.662430763244629}, {'non-toxic': 3.5925912857055664, 'toxic': -3.5325300693511963}, {'bad': 2.8268492221832275, 'medium': -4.111583709716797, 'good': 0.6059072017669678}]\n",
      "text: I really hated the horrible hint towards the end of the film. The film was a lot of fun, and the acting was good. The film was a lot of fun, and the story was interesting. The film was a lot of fun, and the story was interesting. The film\n",
      "reward: [{'NEGATIVE': -1.786222219467163, 'POSITIVE': 1.9841879606246948}, {'NEGATIVE': -3.765641689300537, 'POSITIVE': 4.084320068359375}, {'non-toxic': 3.0286831855773926, 'toxic': -3.231182098388672}, {'bad': 3.1202611923217773, 'medium': -2.5104548931121826, 'good': -1.029152274131775}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.25, 0.75])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rewards_wa_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_query_tensors = get_imdb_query_tensors(bs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.25, 0.75])\n",
    "list_rewards_wa_imdb = predict({\"wa\": wa}, imdb_query_tensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
