{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "from trl.core import LengthSampler\n",
    "import numpy as np\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class ScriptArguments:\n",
    "    sentiment_models = [\n",
    "        \"lvwerra/distilbert-imdb\",\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"martin-ha/toxic-comment-model\",\n",
    "        \"valurank/distilbert-quality\",\n",
    "        \"OpenAssistant/reward-model-deberta-v3-large-v2\",\n",
    "        \"OpenAssistant/reward-model-deberta-v3-base\",\n",
    "        # \"pedropei/sentence-level-certainty\",\n",
    "        # \"ChaiML/gpt2_base_retry_and_continue_5m_reward_model\",\n",
    "        # \"ChaiML/3plus_stars_gpt2_reward\"\n",
    "        # \"CogComp/bart-faithful-summary-detector\"\n",
    "    ]\n",
    "    base_model_name = \"decapoda-research/llama-7b-hf\"\n",
    "    peft_names = [\n",
    "        \"tloen/alpaca-lora-7b\",\n",
    "        \"alexrame/llama-7b-hf-ppo-sentiment-1\"  # python3 gpt-llama-7b-multi-gpu.py --score_goal 1\n",
    "        # \"llama-7b-hf-ppo-sentiment-distilbert-base-ufs2e-1\" # python3 gpt-llama-7b-multi-gpu.py --score_goal 1 --sentiment_model distilbert-base-uncased-finetuned-sst-2-english\n",
    "        # \"llama-7b-hf-ppo-sentiment-distilbert-quality-2\"  #  python3 gpt-llama-7b-multi-gpu.py --sentiment_model valurank/distilbert-quality --score_goal 2\n",
    "        # \"alexrame/llama-7b-hf-ppo-sentiment-reward-model-dvb-0\" # python3 gpt-llama-7b-multi-gpu.py --sentiment_model OpenAssistant/reward-model-deberta-v3-base --score_goal 0\n",
    "        # \"llama-7b-hf-ppo-sentiment-reward-model-dvlv-0\" # python3 gpt-llama-7b-multi-gpu.py --sentiment_model OpenAssistant/reward-model-deberta-v3-large-v2 --score_goal 0\n",
    "    ]\n",
    "\n",
    "    num_samples = 160\n",
    "    every = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_args():\n",
    "        parser = argparse.ArgumentParser(description='Inference')\n",
    "        parser.add_argument(\n",
    "            '--sentiment_models', type=str, nargs='+', default=ScriptArguments.sentiment_models\n",
    "        )\n",
    "        parser.add_argument('--base_model_name', type=str, default=ScriptArguments.base_model_name)\n",
    "        parser.add_argument('--peft_names', type=str, nargs='+', default=ScriptArguments.peft_names)\n",
    "        parser.add_argument('--num_samples', type=int, default=ScriptArguments.num_samples)\n",
    "        parser.add_argument('--every', type=int, default=ScriptArguments.every)\n",
    "        return parser.parse_args()\n",
    "\n",
    "    @staticmethod\n",
    "    def notebook_get_args():\n",
    "        return ScriptArguments()\n",
    "\n",
    "\n",
    "class Instructions:\n",
    "    instruction_llama = \"Generate a movie review.\"\n",
    "    prompt_llama = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    ### Instruction: {instruction_llama}\n",
    "    ### Response: \"\"\"\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_rewards(responses_text):\n",
    "        sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "\n",
    "        def apply_sentiment_pipe(sentiment_pipe, response_text):\n",
    "            if sentiment_pipe.model.name_or_path.startswith(\"OpenAssistant\"):\n",
    "                response_text = sentiment_pipe.tokenizer.cls_token + Instructions.instruction_llama + sentiment_pipe.tokenizer.sep_token + response_text\n",
    "            return sentiment_pipe(response_text, **sent_kwargs)\n",
    "\n",
    "        responses_text = [\n",
    "            response_text.split(\"### Response: \")[-1] for response_text in responses_text\n",
    "        ]\n",
    "        rewards = [\n",
    "            [\n",
    "                apply_sentiment_pipe(sentiment_pipe, response_text)\n",
    "                for sentiment_pipe in sentiment_pipes\n",
    "            ]\n",
    "            for response_text in responses_text\n",
    "        ]\n",
    "\n",
    "        rewards = [Predictor.transform_reward(reward) for reward in rewards]\n",
    "        return rewards\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_reward(reward):\n",
    "        d_reward = []\n",
    "        for rew in reward:\n",
    "            d = {}\n",
    "            assert len(rew) == 1\n",
    "            for r in rew[0]:\n",
    "                d[r[\"label\"]] = r[\"score\"]\n",
    "            d_reward.append(d)\n",
    "        return d_reward\n",
    "\n",
    "    @staticmethod\n",
    "    def average_rewards(rewards):\n",
    "        avg_reward = None\n",
    "        for reward in rewards:\n",
    "            if avg_reward is None:\n",
    "                avg_reward = copy.deepcopy(reward)\n",
    "            else:\n",
    "                for a_dict_reward, r_dict_reward in zip(avg_reward, reward):\n",
    "                    for label in a_dict_reward:\n",
    "                        a_dict_reward[label] = a_dict_reward[label] + r_dict_reward[label]\n",
    "        assert avg_reward is not None\n",
    "        for a_dict_reward in avg_reward:\n",
    "            for label in a_dict_reward:\n",
    "                a_dict_reward[label] = a_dict_reward[label] / len(rewards)\n",
    "        return avg_reward\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prediction_rewards(model, query_tensors):\n",
    "\n",
    "        response_tensors = []\n",
    "        responses_text = []\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        for i in range(len(query_tensors)):\n",
    "            query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "            output = model.generate(\n",
    "                input_ids=query_tensor, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id\n",
    "            ).squeeze()\n",
    "            response_tensors.append(output)\n",
    "            response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            responses_text.append(response)\n",
    "\n",
    "        rewards = Predictor.get_rewards(responses_text)\n",
    "        avg_reward = Predictor.average_rewards(rewards)\n",
    "        return responses_text, rewards, avg_reward\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(dict_models_to_merge, query_tensors, verbose=False):\n",
    "        list_rewards = []\n",
    "        for model_name, model in dict_models_to_merge.items():\n",
    "            responses_text, rewards, avg_reward = Predictor.get_prediction_rewards(\n",
    "                model, query_tensors\n",
    "            )\n",
    "            print(\"=== For model:\", model_name)\n",
    "            for text, reward in zip(responses_text, rewards):\n",
    "                print(\"=== text:\", text, \"reward:\", reward)\n",
    "                if not verbose:\n",
    "                    break\n",
    "            list_rewards.append(avg_reward)\n",
    "        return list_rewards\n",
    "\n",
    "\n",
    "class Samples:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_samples_query_tensors_llama():\n",
    "\n",
    "        list_texts = [\n",
    "            Instructions.prompt_llama + \"I really hated the horrible hint towards\",\n",
    "            Instructions.prompt_llama + \"I really enjoyed the slight hint towards\"\n",
    "        ]\n",
    "\n",
    "        batch = [np.array(tokenizer.encode(text), dtype=np.int32) for text in list_texts]\n",
    "        batch = [b[:-1] for b in batch]\n",
    "        return batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_imdb_query_tensors_llama(bs=16):\n",
    "        ds = load_dataset(\"imdb\", split=\"test\")\n",
    "        ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "        input_min_text_length = 2\n",
    "        input_max_text_length = 8\n",
    "        size_prompt_llama = len(tokenizer.encode(Instructions.prompt_llama))\n",
    "        input_size = LengthSampler(\n",
    "            size_prompt_llama + input_min_text_length, size_prompt_llama + input_max_text_length\n",
    "        )\n",
    "\n",
    "        def tokenize(sample):\n",
    "            sample[\"input_ids\"] = tokenizer.encode(Instructions.prompt_llama +\n",
    "                                                   sample[\"text\"])[:input_size()]\n",
    "            sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "            return sample\n",
    "\n",
    "        ds = ds.map(tokenize, batched=False)\n",
    "        ds.set_format(type=\"torch\")\n",
    "\n",
    "        #### get a batch from the dataset\n",
    "        ds.set_format(\"pandas\")\n",
    "        df_batch = ds[:].sample(bs)\n",
    "        query_tensors = df_batch['input_ids'].tolist()\n",
    "        return query_tensors\n",
    "\n",
    "\n",
    "class Pipelines:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pipes(sentiment_models):\n",
    "        print(f\"Load sentiment model with {sentiment_models}\")\n",
    "        sentiment_pipes = [\n",
    "            pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=sentiment_model,\n",
    "                device=device,\n",
    "                tokenizer=sentiment_model if \"ChaiML\" not in sentiment_model else \"gpt2\"\n",
    "            ) for sentiment_model in sentiment_models\n",
    "        ]\n",
    "        return sentiment_pipes\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_tokenizer(base_model_name):\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\n",
    "            base_model_name, add_eos_token=True, padding_side=\"left\"\n",
    "        )\n",
    "        tokenizer.pad_token_id = 0\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "class Loader:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_base_model(base_model_name):\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model_name, load_in_8bit=True, device_map=\"auto\"\n",
    "        )\n",
    "        return base_model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_peft_model(base_model, peft_name):\n",
    "        peft_model = PeftModel.from_pretrained(base_model, peft_name)\n",
    "        peft_model.eval()\n",
    "        return peft_model\n",
    "\n",
    "LOAD_ONLY_LORA = True\n",
    "class WeightAverager:\n",
    "\n",
    "    @staticmethod\n",
    "    def average_weights(base_model, peft_names, coefficients):\n",
    "        weights_averaged = OrderedDict()\n",
    "        i = 0\n",
    "        for peft_name, coefficient in zip(peft_names, coefficients):\n",
    "            if coefficient == 0.:\n",
    "                continue\n",
    "            current_model = Loader.load_peft_model(base_model, peft_name)\n",
    "            current_weights = current_model.state_dict()\n",
    "            for key in list(current_weights.keys()):\n",
    "                if LOAD_ONLY_LORA and \"lora\" not in key:\n",
    "                    pass\n",
    "                elif i == 0:\n",
    "                    weights_averaged[key] = coefficient * current_weights[key]\n",
    "                else:\n",
    "                    weights_averaged[key] += coefficient * current_weights[key]\n",
    "                del current_weights[key]\n",
    "            del current_model\n",
    "            torch.cuda.empty_cache()\n",
    "            i += 1\n",
    "        return weights_averaged\n",
    "\n",
    "    @staticmethod\n",
    "    def build_wa(base_model, peft_names, coefficients):\n",
    "        weights_averaged = WeightAverager.average_weights(\n",
    "            base_model=base_model, peft_names=peft_names, coefficients=coefficients\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        wa = Loader.load_peft_model(base_model, peft_names[0])\n",
    "        wa.load_state_dict(weights_averaged, strict=not LOAD_ONLY_LORA)\n",
    "        return wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load LMs with ['tloen/alpaca-lora-7b', 'alexrame/llama-7b-hf-ppo-sentiment-1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:11<00:00,  2.95it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentiment model with ['lvwerra/distilbert-imdb', 'distilbert-base-uncased-finetuned-sst-2-english', 'martin-ha/toxic-comment-model', 'valurank/distilbert-quality', 'OpenAssistant/reward-model-deberta-v3-large-v2', 'OpenAssistant/reward-model-deberta-v3-base']\n"
     ]
    }
   ],
   "source": [
    "script_args = ScriptArguments.notebook_get_args()\n",
    "script_args.num_samples = -1\n",
    "\n",
    "assert \"llama\" in script_args.base_model_name.lower()\n",
    "print(f\"Load LMs with {script_args.peft_names}\")\n",
    "\n",
    "# 1. load all key components\n",
    "base_model = Loader.load_base_model(script_args.base_model_name)\n",
    "tokenizer = Tokenizer.load_tokenizer(script_args.base_model_name)\n",
    "sentiment_pipes = Pipelines.load_pipes(script_args.sentiment_models)\n",
    "\n",
    "# 2. load dataset\n",
    "if script_args.num_samples == -1:\n",
    "    query_tensors = Samples.get_samples_query_tensors_llama()\n",
    "else:\n",
    "    query_tensors = Samples.get_imdb_query_tensors_llama(bs=script_args.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_ONLY_LORA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 24 14:57:04 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  RTX A6000           On   | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 30%   31C    P8    28W / 300W |  11110MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A   1399127      C   ...onda3/envs/nlp/bin/python    11107MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "coeff = 0.2\n",
    "# 4.1 load wa\n",
    "base_model=base_model\n",
    "peft_names=script_args.peft_names\n",
    "coefficients=[1 - coeff, coeff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_averaged = WeightAverager.average_weights(\n",
    "    base_model=base_model, peft_names=peft_names, coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 24 14:57:27 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  RTX A6000           On   | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 30%   33C    P2    76W / 300W |  11158MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A   1399127      C   ...onda3/envs/nlp/bin/python    11155MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weights_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 predict with wa\n",
    "list_rewards_wa = Predictor.predict({\"wa coeff \" + str(coeff): wa}, query_tensors)\n",
    "print(\"== coeff\", coeff, list_rewards_wa[0], \"\\n\")\n",
    "dict_coeff_to_reward[coeff] = list_rewards_wa[0]\n",
    "\n",
    "# 4.3 del wa\n",
    "del wa\n",
    "torch.cuda.empty_cache()\n",
    "wa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rewards_wa = Predictor.predict({\"wa coeff \" + str(coeff): wa}, query_tensors)\n",
    "print(\"== coeff\", coeff, list_rewards_wa[0], \"\\n\")\n",
    "dict_coeff_to_reward[coeff] = list_rewards_wa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = dict_models_to_merge[\"tloen/alpaca-lora-7b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = current_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_weights = current_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff=0.2\n",
    "weights_averaged = WeightAverager.average_weights(\n",
    "    list_models=dict_models_to_merge.values(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_averaged = OrderedDict()\n",
    "coeff = 0.2\n",
    "coefficients=[1 - coeff, coeff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(current_weights.keys()):\n",
    "    if i == 0:\n",
    "        weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "    else:\n",
    "        weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "    del current_weights[key]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wa\n",
    "torch.cuda.empty_cache()\n",
    "wa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        script_args.base_model_name, add_eos_token=True, padding_side=\"left\"\n",
    "    )\n",
    "    tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipelines:\n",
    "\n",
    "    def load_pipes(sentiment_models):\n",
    "        print(f\"Load sentiment model with {sentiment_models}\")\n",
    "        sentiment_pipes = [\n",
    "            pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=sentiment_model,\n",
    "                device=device,\n",
    "                tokenizer=sentiment_model if \"ChaiML\" not in sentiment_model else \"gpt2\"\n",
    "            ) for sentiment_model in sentiment_models\n",
    "        ]\n",
    "        return sentiment_pipes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipes = Pipelines.load_pipes(script_args.sentiment_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=sentiment_model,\n",
    "        device=device,\n",
    "        tokenizer=sentiment_model if \"ChaiML\" not in sentiment_model else \"gpt2\"\n",
    "    ) for sentiment_model in script_args.sentiment_models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models_to_merge = get_dict_models()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    script_args.base_model_name, add_eos_token=True, padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=sentiment_model,\n",
    "        device=device,\n",
    "        tokenizer=sentiment_model if \"ChaiML\" not in sentiment_model else \"gpt2\"\n",
    "    ) for sentiment_model in script_args.sentiment_models\n",
    "]\n",
    "\n",
    "samples_query_tensors = get_samples_query_tensors_llama()\n",
    "list_rewards_samples = predict(dict_models_to_merge, samples_query_tensors, verbose=True)\n",
    "for rewards_samples in list_rewards_samples:\n",
    "    print(rewards_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(a,b, \"\\n\") for a, b in zip(script_args.sentiment_models, list_rewards_samples[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args.num_samples=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_query_tensors = samples_query_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rewards_imdb = predict(dict_models_to_merge, imdb_query_tensors, verbose=False)\n",
    "dict_coeff_to_reward = {}\n",
    "dict_coeff_to_reward[0] = list_rewards_imdb[0]\n",
    "if len(dict_coeff_to_reward) > 1:\n",
    "    dict_coeff_to_reward[1] = list_rewards_imdb[1]\n",
    "for model_name, rewards_imdb in zip(dict_models_to_merge.keys(), list_rewards_imdb):\n",
    "    print(model_name)\n",
    "    print(rewards_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff= 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wa\n",
    "torch.cuda.empty_cache()\n",
    "dict_models_to_merge = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dict_models_to_merge is None:\n",
    "    dict_models_to_merge = get_dict_models()\n",
    "wa = enrich_wa_states(dict_models_to_merge, coefficients=[1 - coeff, coeff])\n",
    "list_rewards_wa_imdb = predict({\"wa\": wa}, imdb_query_tensors)\n",
    "print(coeff)\n",
    "print(list_rewards_wa_imdb)\n",
    "dict_coeff_to_reward[coeff] = list_rewards_wa_imdb[0]\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bullshit is what I know\"\n",
    "list_rewards = get_rewards([text])\n",
    "[print(a,b, \"\\n\") for a, b in zip(script_args.sentiment_models, list_rewards[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n",
    "question, answer = \n",
    "inputs = tokenizer(question, answer, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_llama = \"Generate a movie review.\"\n",
    "prompt_llama = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction: {instruction_llama}\n",
    "### Response: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{key: value for key, value in sentiment_pipes[-2].__dict__.items() if type(value)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_llama = \"Explain nuclear fusion like I am five\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = sentiment_pipes[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text_clean = instruction_llama + response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe.model.name_or_path.startswith(\"OpenAssistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text_clean = sentiment_pipe.tokenizer.cls_token + \" \" + instruction_llama + sentiment_pipe.tokenizer.sep_token + response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipes[-2](response_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_to_rlhf = tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_to_rlhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(2.2720)/(1+math.exp(2.2720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rank_model(input_ids=inputs[\"input_ids\"])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rewards_imdb = predict(dict_models_to_merge, imdb_query_tensors[:3], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rewards_imdb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_query_tensors():\n",
    "    list_texts = [\n",
    "        \"I really enjoyed the slight hint towards\",\n",
    "        \"I really hated the horrible hint towards\"\n",
    "    ]\n",
    "\n",
    "    batch = [np.array(tokenizer.encode(text)[:-1], dtype=np.int32) for text in list_texts]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(script_args.model_names[0], add_eos_token=True, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_query_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_rewards(model, query_tensors):\n",
    "    def get_rewards(responses_text):\n",
    "        sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "        responses_text = [response_text.split(\"### Response: \")[-1] for response_text in responses_text]\n",
    "        print(\"responses_text cleant\", responses_text)\n",
    "        rewards = [\n",
    "            [sentiment_pipe(response_text, **sent_kwargs) for sentiment_pipe in sentiment_pipes]\n",
    "            for response_text in responses_text]\n",
    "\n",
    "        rewards = [transform_reward(reward) for reward in rewards]\n",
    "        return rewards\n",
    "    def transform_reward(reward):\n",
    "        d_reward = []\n",
    "        for rew in reward:\n",
    "            d = {}\n",
    "            assert len(rew) == 1\n",
    "            for r in rew[0]:\n",
    "                d[r[\"label\"]] = r[\"score\"]\n",
    "            d_reward.append(d)\n",
    "        return d_reward\n",
    "\n",
    "    def average_rewards(rewards):\n",
    "        avg_reward = None\n",
    "        for reward in rewards:\n",
    "            if avg_reward is None:\n",
    "                avg_reward = copy.deepcopy(reward)\n",
    "            else:\n",
    "                for a_dict_reward, r_dict_reward in zip(avg_reward, reward):\n",
    "                    for label in a_dict_reward:\n",
    "                        a_dict_reward[label] = a_dict_reward[label] + r_dict_reward[label]\n",
    "\n",
    "        for a_dict_reward in avg_reward:\n",
    "            for label in a_dict_reward:\n",
    "                a_dict_reward[label] = a_dict_reward[label] / len(rewards)\n",
    "        return avg_reward\n",
    "\n",
    "    response_tensors = []\n",
    "    responses_text = []\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    for i in range(len(query_tensors)):\n",
    "        query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "        output = model.generate(\n",
    "            input_ids=query_tensor, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id\n",
    "        ).squeeze()\n",
    "        response_tensors.append(output)\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        responses_text.append(response)\n",
    "\n",
    "    rewards = get_rewards(responses_text)\n",
    "    avg_reward = average_rewards(rewards)\n",
    "    return responses_text, rewards, avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_query_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_query_tensors_llama():\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    ### Instruction: Tell me your opinion about this movie.\n",
    "    ### Response: \"\"\"\n",
    "    list_texts = [\n",
    "        prompt + \"I really enjoyed the slight hint towards\",\n",
    "        prompt + \"I really hated the horrible hint towards\"\n",
    "    ]\n",
    "\n",
    "    batch = [np.array(tokenizer.encode(text), dtype=np.int32) for text in list_texts]\n",
    "    batch = [b[:-1] for b in batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llama = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction: Tell me your opinion about this movie.\n",
    "### Response: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_prompt_llama = len(tokenizer.encode(prompt_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_query_tensors_llama(bs=16):\n",
    "    ds = load_dataset(\"imdb\", split=\"test\")\n",
    "    ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "    input_min_text_length=2\n",
    "    input_max_text_length=8\n",
    "    size_prompt_llama = len(tokenizer.encode(prompt_llama))\n",
    "    input_size = LengthSampler(size_prompt_llama + input_min_text_length, size_prompt_llama + input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt_llama + sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    #### get a batch from the dataset\n",
    "    ds.set_format(\"pandas\")\n",
    "    df_batch = ds[:].sample(bs)\n",
    "    query_tensors = df_batch['input_ids'].tolist()\n",
    "    return query_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_prompt_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_query_tensors = get_samples_query_tensors_llama()\n",
    "list_rewards_samples = predict(dict_models_to_merge, samples_query_tensors, verbose=True)\n",
    "for rewards_samples in list_rewards_samples:\n",
    "    print(rewards_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(imdb_query_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_query_tensors = get_imdb_query_tensors_llama(bs=8)\n",
    "list_rewards_imdb = predict(dict_models_to_merge, imdb_query_tensors, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_coeff_to_reward = {}\n",
    "dict_coeff_to_reward[0] = list_rewards_imdb[0]\n",
    "for model_name, rewards_imdb in zip(dict_models_to_merge.keys(), list_rewards_imdb):\n",
    "    print(model_name)\n",
    "    print(rewards_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rewards_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
