{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(good_score[0][:, 1] > bad_score[0][:, 1]) # True, label mapping: \"0\" -> \"Hallucinated\" \"1\" -> \"Faithful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification\n",
    "tokenizer_tristan = AutoTokenizer.from_pretrained(\"Tristan/gpt2_reward_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad tensor([0.5631])\n",
      "good tensor([0.5590])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pair = tokenizer(text=bad_summary, text_pair=article, return_tensors='pt')\n",
    "good_pair = tokenizer(text=good_summary, text_pair=article, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"Tristan/gpt2_reward_summarization\")\n",
    "\n",
    "bad_score = model(**bad_pair)\n",
    "good_score = model(**good_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tristan??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tristan.decode(good_pair[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(\"sugam11/gpt2-rlhf-reward\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "reward_name = \"OpenAssistant/reward-model-electra-large-discriminator\"\n",
    "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n",
    "question, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, HfArgumentParser, pipeline\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, PeftModel\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use trl with accelerate.\n",
    "#\n",
    "# This example fine-tunes a GPT2 model on the IMDB dataset using PPO\n",
    "# (proximal policy optimization).\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#\n",
    "# To run it in each of these various modes, first initialize the accelerate\n",
    "# configuration with `accelerate config`\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "########################################################################\n",
    "# NOTE for to train with a 8-bit model a more recent version of\n",
    "# transformers is required, full dependecies for this example:\n",
    "# pip install  bitsandbytes datasets accelerate loralib\n",
    "# pip install  git+https://github.com/huggingface/transformers.git@main\n",
    "# pip install peft\n",
    "########################################################################\n",
    "\n",
    "# If you want to log with tensorboard, add the kwarg\n",
    "# `accelerator_kwargs={\"logging_dir\": PATH_TO_LOGS}` to the PPOConfig.\n",
    "\n",
    "\n",
    "class DefaultArgs:\n",
    "    learning_rate = 1.41e-5\n",
    "    sentiment_model = \"lvwerra/distilbert-imdb\"\n",
    "    model_name = \"decapoda-research/llama-7b-hf\"\n",
    "    lora_config = \"tloen/alpaca-lora-7b\"\n",
    "    log_with = \"\"\n",
    "    logs = \"\"\n",
    "    prompt = \"medium\"\n",
    "    score_goal = \"1\"\n",
    "    mini_batch_size = 4\n",
    "    batch_size = 128\n",
    "    gradient_accumulation_steps = 1\n",
    "    init_kl_coef = 0.2\n",
    "    adap_kl_ctrl = 1\n",
    "    ref_model = None\n",
    "    output_min_length = 18\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with PPO\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: Optional[str] = field(\n",
    "        default=DefaultArgs.model_name, metadata={\"help\": \"the model name\"}\n",
    "    )\n",
    "    # decapoda-research/llama-13b-hf\n",
    "    #\n",
    "    lora_config: Optional[str] = field(\n",
    "        default=DefaultArgs.lora_config, metadata={\"help\": \"the lora config\"}\n",
    "    )\n",
    "    # chansung/alpaca-lora-13b\n",
    "    log_with: Optional[str] = field(\n",
    "        default='wandb', metadata={\"help\": \"use 'wandb' to log with wandb\"}\n",
    "    )\n",
    "    logs: Optional[str] = field(default=DefaultArgs.logs, metadata={\"help\": \"logs\"})\n",
    "    prompt: Optional[str] = field(default=DefaultArgs.prompt, metadata={\"help\": \"prompt\"})\n",
    "    learning_rate: Optional[float] = field(\n",
    "        default=DefaultArgs.learning_rate, metadata={\"help\": \"the learning rate\"}\n",
    "    )\n",
    "\n",
    "    sentiment_model: Optional[str] = field(\n",
    "        default=DefaultArgs.sentiment_model, metadata={\"help\": \"which sentiment model to use\"}\n",
    "    )\n",
    "\n",
    "    score_goal: Optional[str] = field(\n",
    "        default=DefaultArgs.score_goal, metadata={\"help\": \"which sentiment to use\"}\n",
    "    )\n",
    "    mini_batch_size: Optional[int] = field(\n",
    "        default=DefaultArgs.mini_batch_size, metadata={\"help\": \"the PPO minibatch size\"})\n",
    "    batch_size: Optional[int] = field(\n",
    "        default=DefaultArgs.batch_size, metadata={\"help\": \"the batch size\"})\n",
    "    init_kl_coef: Optional[float] = field(\n",
    "        default=DefaultArgs.init_kl_coef, metadata={\"help\": \"init_kl_coef\"})\n",
    "    adap_kl_ctrl: Optional[int] = field(\n",
    "        default=DefaultArgs.adap_kl_ctrl, metadata={\"help\": \"adap_kl_ctrl\"}\n",
    "    )\n",
    "    output_min_length: Optional[int] = field(\n",
    "        default=DefaultArgs.output_min_length, metadata={\"help\": \"output_min_length\"})\n",
    "    ref_model: Optional[str] = field(\n",
    "        default=DefaultArgs.ref_model, metadata={\"help\": \"the ref_model. Can be None or copy\"}\n",
    "    )\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=DefaultArgs.gradient_accumulation_steps, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "\n",
    "\n",
    "class Loader:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_base_model(base_model_name):\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model_name, load_in_8bit=True, device_map=\"auto\"\n",
    "        )\n",
    "        base_model = prepare_model_for_int8_training(base_model)\n",
    "        return base_model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_peft_model(base_model, lora_config):\n",
    "        \"\"\"### Apply LoRA\n",
    "        Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`.\n",
    "        \"\"\"\n",
    "        if lora_config in [None, \"none\"]:\n",
    "            lora_config_loaded = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.05,\n",
    "                target_modules=None,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "            )\n",
    "            model = get_peft_model(base_model, lora_config_loaded)\n",
    "            lora_model = model\n",
    "        elif lora_config == \"nolora\":\n",
    "            raise ValueError()\n",
    "            # model = base_model\n",
    "            # lora_model = PeftModel.from_pretrained(model, DefaultArgs.lora_config)\n",
    "        else:\n",
    "            model = PeftModel.from_pretrained(base_model, lora_config)\n",
    "            lora_model = model\n",
    "        modelvaluehead = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "        return modelvaluehead, lora_model\n",
    "\n",
    "    @staticmethod\n",
    "    def print_trainable_parameters(model):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    @staticmethod\n",
    "    def load_tokenizer(base_model_name):\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(base_model_name, add_eos_token=True)\n",
    "        tokenizer.pad_token_id = 0\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "if False:\n",
    "    parser = HfArgumentParser(ScriptArguments)\n",
    "    script_args = parser.parse_args_into_dataclasses()[0]\n",
    "else:\n",
    "    script_args = DefaultArgs()\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=script_args.model_name,\n",
    "    init_kl_coef=script_args.init_kl_coef,\n",
    "    adap_kl_ctrl=script_args.adap_kl_ctrl,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    batch_size=script_args.batch_size,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    optimize_cuda_cache=True,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "\n",
    "base_model = Loader.load_base_model(script_args.model_name)\n",
    "model, _ = Loader.load_peft_model(base_model, lora_config=script_args.lora_config)\n",
    "tokenizer = Tokenizer.load_tokenizer(script_args.model_name)\n",
    "Loader.print_trainable_parameters(model)\n",
    "\n",
    "class Instructions:\n",
    "    instructions_summary = [\n",
    "        \"Read the following text and generate a 1 short sentence summary.\",\n",
    "        \"Write a concise summary of a fact.\", \"Write a concise summary of the provided text.\",\n",
    "        \"Create a summary of the text below\", \"Generate a summary.\",\n",
    "        \"Write a one-sentence summary of the following news article.\",\n",
    "        \"Construct a concise summary of the following article.\",\n",
    "        \"Generate a one-sentence summary of the story.\",\n",
    "        \"Analyze the following news article and provide a brief summary.\",\n",
    "        \"Write a summary of the given article. Make sure it is less than 100 words and it accurately reflects the key points of the article.\"\n",
    "    ]\n",
    "    instruction_movie = \"Generate a movie review.\"\n",
    "\n",
    "    if script_args.prompt == \"long\":\n",
    "        prompt_movie = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "            ### Instruction: {Instructions.instruction_movie}\n",
    "            ### Response: \"\"\"\n",
    "    elif script_args.prompt == \"hhh\":\n",
    "        prompt_movie = \"### Instruction: Generate an helpful harmless honest movie review. ### Response: \"\n",
    "    elif script_args.prompt == \"medium\":\n",
    "        prompt_movie = \"### Instruction: Generate a movie review. ### Response: \"\n",
    "    else:\n",
    "        prompt_movie = \"Instruction: Generate a movie review. Response: \"\n",
    "    size_prompt_movie = len(tokenizer.encode(prompt_movie)) - 1\n",
    "\n",
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(\n",
    "    config, tokenizer, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(\n",
    "        Instructions.size_prompt_movie + input_min_text_length,\n",
    "        Instructions.size_prompt_movie + input_max_text_length\n",
    "    )\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(Instructions.prompt_movie +\n",
    "                                               sample[\"review\"])[:input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(config, tokenizer=tokenizer)\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate\n",
    ")\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    parser = HfArgumentParser(ScriptArguments)\n",
    "    script_args = parser.parse_args_into_dataclasses()[0]\n",
    "else:\n",
    "    script_args = NotebookArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Loader.load_base_model(script_args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.is_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, lora_model = Loader.load_peft_model(base_model, lora_config=\"nolora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.is_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args = ScriptArguments.notebook_get_args()\n",
    "assert \"llama\" in script_args.base_model_name.lower()\n",
    "print(f\"Load LMs with {script_args.peft_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Loader.load_base_model(script_args.base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = base_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = base_model_v2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[\"model.layers.2.self_attn.rotary_emb.inv_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_v2 = LlamaForCausalLM.from_pretrained(\n",
    "            script_args.peft_names[0],     return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Load LMs with {script_args.model_names}\")\n",
    "dict_models_to_merge = OrderedDict({model_name: load_model(model_name) for model_name in script_args.model_names})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PeftConfig.from_pretrained(script_args.model_names[0]).base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\"sentiment-analysis\", model=sentiment_model, device=device)\n",
    "    for sentiment_model in script_args.sentiment_models]\n",
    "\n",
    "list_states_dict = []\n",
    "for current_model in dict_models_to_merge.values():\n",
    "    current_weights = copy.deepcopy(current_model.state_dict())\n",
    "    list_states_dict.append(current_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_query_tensors = get_samples_query_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.3, 0.7])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.25, 0.75])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for coeff in [x / 20 for x in range(17, -1, -1)]:\n",
    "    wa = enrich_wa_states(list_states_dict, coefficients=[1 - coeff, coeff])\n",
    "    list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors)\n",
    "    print(coeff)\n",
    "    print(list_rewards_wa_samples)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
