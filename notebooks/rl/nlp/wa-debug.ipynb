{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptArguments:\n",
    "    sentiment_models = [\n",
    "        \"lvwerra/distilbert-imdb\", \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"martin-ha/toxic-comment-model\", \"valurank/distilbert-quality\"\n",
    "    ]\n",
    "    model_names = [\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model distilbert-base-uncased-finetuned-sst-2-english\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-neg-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model distilbert-base-uncased-finetuned-sst-2-english --score_goal negative\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-toxic-neg-lr1.41e-05\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model martin-ha/toxic-comment-model --score_goal 1\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-toxic-0\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model martin-ha/toxic-comment-model --score_goal 0,\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-1\",\n",
    "        # cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model valurank/distilbert-quality --score_goal 1\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2\"\n",
    "        #  cli: accelerate launch gpt-neo-20b_sentiment_peft.py --sentiment_model valurank/distilbert-quality --score_goal 2\n",
    "    ]\n",
    "    num_samples = 160\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Inference')\n",
    "    parser.add_argument('--sentiment_models', type=str, nargs='+', default=ScriptArguments.sentiment_models)\n",
    "    parser.add_argument('--model_names', type=str, nargs='+', default=ScriptArguments.model_names)\n",
    "    parser.add_argument('--num_samples', type=int, default=ScriptArguments.num_samples)\n",
    "    return parser.parse_args()\n",
    "\n",
    "def notebook_get_args():\n",
    "    return ScriptArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(peft_model_id):\n",
    "    peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        return_dict=True,\n",
    "        #torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # Load the Lora model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        peft_model_id,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# average\n",
    "def average_weights(input_models, coefficients):\n",
    "    \"\"\"average weights of different transformer models based on the amount of training data they were trained on\"\"\"\n",
    "    weights_averaged = OrderedDict()\n",
    "    for i, current_model in tqdm(enumerate(input_models), leave=False):\n",
    "        current_weights = current_model.state_dict()\n",
    "        for key in current_weights.keys():\n",
    "            if i == 0:\n",
    "                weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "            else:\n",
    "                weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "\n",
    "    return weights_averaged\n",
    "\n",
    "def enrich_wa(dict_models_to_merge, coefficients=None):\n",
    "    if coefficients is None:\n",
    "        coefficients = [1 / len(dict_models_to_merge) for _ in len(dict_models_to_merge)]\n",
    "    weights_averaged = average_weights(dict_models_to_merge.values(), coefficients)\n",
    "    base_model_copy = list(dict_models_to_merge.values())[0]\n",
    "    base_model_copy.load_state_dict(weights_averaged, strict=True)\n",
    "    return base_model_copy\n",
    "\n",
    "\n",
    "def get_prediction_rewards(model, query_tensors):\n",
    "    def get_rewards(responses_text):\n",
    "        sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "        rewards = [\n",
    "            [sentiment_pipe(response_text, **sent_kwargs) for sentiment_pipe in sentiment_pipes]\n",
    "            for response_text in responses_text]\n",
    "\n",
    "        rewards = [transform_reward(reward) for reward in rewards]\n",
    "        return rewards\n",
    "    def transform_reward(reward):\n",
    "        d_reward = []\n",
    "        for rew in reward:\n",
    "            d = {}\n",
    "            assert len(rew) == 1\n",
    "            for r in rew[0]:\n",
    "                d[r[\"label\"]] = r[\"score\"]\n",
    "            d_reward.append(d)\n",
    "        return d_reward\n",
    "\n",
    "    def average_rewards(rewards):\n",
    "        avg_reward = None\n",
    "        for reward in rewards:\n",
    "            if avg_reward is None:\n",
    "                avg_reward = copy.deepcopy(reward)\n",
    "            else:\n",
    "                for a_dict_reward, r_dict_reward in zip(avg_reward, reward):\n",
    "                    for label in a_dict_reward:\n",
    "                        a_dict_reward[label] = a_dict_reward[label] + r_dict_reward[label]\n",
    "\n",
    "        for a_dict_reward in avg_reward:\n",
    "            for label in a_dict_reward:\n",
    "                a_dict_reward[label] = a_dict_reward[label] / len(rewards)\n",
    "        return avg_reward\n",
    "\n",
    "    response_tensors = []\n",
    "    responses_text = []\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    for i in range(len(query_tensors)):\n",
    "        query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "        output = model.generate(\n",
    "            input_ids=query_tensor, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id\n",
    "        ).squeeze()\n",
    "        response_tensors.append(output)\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        responses_text.append(response)\n",
    "\n",
    "    rewards = get_rewards(responses_text)\n",
    "    avg_reward = average_rewards(rewards)\n",
    "    return responses_text, rewards, avg_reward\n",
    "\n",
    "\n",
    "def get_samples_query_tensors():\n",
    "    list_texts = [\n",
    "        \"I really enjoyed the slight hint towards\",\n",
    "        \"I really hated the horrible hint towards\"\n",
    "    ]\n",
    "\n",
    "    batch = tokenizer(list_texts, return_tensors=\"pt\")\n",
    "    return batch[\"input_ids\"]\n",
    "\n",
    "\n",
    "def predict(dict_models_to_merge, query_tensors, verbose=False):\n",
    "    list_rewards = []\n",
    "    for model_name, model in dict_models_to_merge.items():\n",
    "        responses_text, rewards, avg_reward = get_prediction_rewards(model, query_tensors)\n",
    "        print(\"model:\", model_name)\n",
    "        print(\"responses_text[0]\", responses_text[0])\n",
    "        if verbose:\n",
    "            print(\"avg reward:\", avg_reward)\n",
    "            for text, reward in zip(responses_text, rewards):\n",
    "                print(\"text:\", text)\n",
    "                print(\"reward:\", reward)\n",
    "            print(\"\\n\")\n",
    "        list_rewards.append(avg_reward)\n",
    "    return list_rewards\n",
    "\n",
    "\n",
    "\n",
    "def get_imdb_query_tensors(bs=16):\n",
    "    ds = load_dataset(\"imdb\", split=\"test\")\n",
    "    ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "    input_min_text_length=2\n",
    "    input_max_text_length=8\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    #### get a batch from the dataset\n",
    "    ds.set_format(\"pandas\")\n",
    "    df_batch = ds[:].sample(bs)\n",
    "    query_tensors = df_batch['input_ids'].tolist()\n",
    "    return query_tensors\n",
    "\n",
    "# samples_query_tensors = get_samples_query_tensors()\n",
    "\n",
    "def average_states_dict(list_states_dict, coefficients):\n",
    "    \"\"\"average weights of different transformer models based on the amount of training data they were trained on\"\"\"\n",
    "    weights_averaged = OrderedDict()\n",
    "    for i, current_weights in enumerate(list_states_dict):\n",
    "        for key in current_weights.keys():\n",
    "            if i == 0:\n",
    "                weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "            else:\n",
    "                weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "    return weights_averaged\n",
    "\n",
    "\n",
    "def enrich_wa_states(list_states_dict, coefficients=None):\n",
    "    weights_averaged = average_states_dict(list_states_dict, coefficients)\n",
    "    base_model_copy = list(dict_models_to_merge.values())[0]\n",
    "    base_model_copy.load_state_dict(weights_averaged, strict=True)\n",
    "    return base_model_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args = notebook_get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load LMs with ['alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05', 'alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentiment model with ['lvwerra/distilbert-imdb', 'distilbert-base-uncased-finetuned-sst-2-english', 'martin-ha/toxic-comment-model', 'valurank/distilbert-quality']\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Load LMs with {script_args.model_names}\")\n",
    "dict_models_to_merge = OrderedDict({model_name: load_model(model_name) for model_name in script_args.model_names})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PeftConfig.from_pretrained(script_args.model_names[0]).base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\"sentiment-analysis\", model=sentiment_model, device=device)\n",
    "    for sentiment_model in script_args.sentiment_models]\n",
    "\n",
    "list_states_dict = []\n",
    "for current_model in dict_models_to_merge.values():\n",
    "    current_weights = copy.deepcopy(current_model.state_dict())\n",
    "    list_states_dict.append(current_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_query_tensors = get_samples_query_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rame/ipykernel_3350526/440160295.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:195: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: wa\n",
      "responses_text[0] I really enjoyed the slight hint towards the end of the film. The film was very well done and the characters were well-developed. The film was very well-balanced and the story was well-told. The film was well-done and the story was well-told. The\n",
      "avg reward: [{'NEGATIVE': -0.06881940364837646, 'POSITIVE': 0.010891556739807129}, {'NEGATIVE': 0.1752924919128418, 'POSITIVE': 0.43285930156707764}, {'non-toxic': 2.6703314185142517, 'toxic': -2.844227433204651}, {'bad': 2.0736796855926514, 'medium': -1.498795136809349, 'good': -0.987948015332222}]\n",
      "text: I really enjoyed the slight hint towards the end of the film. The film was very well done and the characters were well-developed. The film was very well-balanced and the story was well-told. The film was well-done and the story was well-told. The\n",
      "reward: [{'NEGATIVE': -2.524400472640991, 'POSITIVE': 2.78214693069458}, {'NEGATIVE': -4.2379865646362305, 'POSITIVE': 4.612117290496826}, {'non-toxic': 3.5791893005371094, 'toxic': -3.5277674198150635}, {'bad': 2.236325740814209, 'medium': -2.6638107299804688, 'good': -0.09869375824928284}]\n",
      "text: I really hated the horrible hint towards the end of the film. The film was a bit of a disappointment, and the film was a bit of a disappointment. The film was a bit of a disappointment, and the film was a bit of a disappointment. The film was a bit of\n",
      "reward: [{'NEGATIVE': 2.3867616653442383, 'POSITIVE': -2.760363817214966}, {'NEGATIVE': 4.588571548461914, 'POSITIVE': -3.746398687362671}, {'non-toxic': 1.761473536491394, 'toxic': -2.1606874465942383}, {'bad': 1.9110336303710938, 'medium': -0.33377954363822937, 'good': -1.8772022724151611}]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.3, 0.7])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rame/ipykernel_3350526/440160295.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: wa\n",
      "responses_text[0] I really enjoyed the slight hint towardsdigyffitiracistNazisstrousicesterSTDOUTizarreSTDOUTizarreSTDOUTizarreSTDOUTfascistaghettifascistaghettifascistpherdracistoilerlbsikiniNazisstrousfasciststrousfasciststrousfasciststrousikini Imranuggishffitiuggishfasciststrouscorruptionampunkikini othersuggishffitiuggishffitiuggishffitiuggishffiti\n",
      "avg reward: [{'NEGATIVE': 0.6124661266803741, 'POSITIVE': -0.7730308771133423}, {'NEGATIVE': 2.5565974712371826, 'POSITIVE': -2.25279837846756}, {'non-toxic': 1.3001094087958336, 'toxic': -1.5670802146196365}, {'bad': 1.868288278579712, 'medium': -2.549556255340576, 'good': 0.10077637434005737}]\n",
      "text: I really enjoyed the slight hint towardsdigyffitiracistNazisstrousicesterSTDOUTizarreSTDOUTizarreSTDOUTizarreSTDOUTfascistaghettifascistaghettifascistpherdracistoilerlbsikiniNazisstrousfasciststrousfasciststrousfasciststrousikini Imranuggishffitiuggishfasciststrouscorruptionampunkikini othersuggishffitiuggishffitiuggishffitiuggishffiti\n",
      "reward: [{'NEGATIVE': -0.6406882405281067, 'POSITIVE': 0.6722524166107178}, {'NEGATIVE': 1.8547003269195557, 'POSITIVE': -1.6694592237472534}, {'non-toxic': 2.419613838195801, 'toxic': -2.7737743854522705}, {'bad': 1.3983044624328613, 'medium': -2.5268750190734863, 'good': 0.5206782221794128}]\n",
      "text: I really hated the horrible hint towardslbsikiniicesterizarreSTDOUTfasciststrousfascistpherdNazisstrousfascistaghettiikini Imranuggishikini Imranuggishfasciststrousfasciststrousfasciststrousfasciststrousfascistuggishfascistuggishfascistossalikini Imranuggishffitiuggishffitiomsdayikini Imranikini Imranikini Imranikini Imranikini Imran\n",
      "reward: [{'NEGATIVE': 1.865620493888855, 'POSITIVE': -2.2183141708374023}, {'NEGATIVE': 3.2584946155548096, 'POSITIVE': -2.836137533187866}, {'non-toxic': 0.1806049793958664, 'toxic': -0.36038604378700256}, {'bad': 2.3382720947265625, 'medium': -2.572237491607666, 'good': -0.3191254734992981}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wa = enrich_wa_states(list_states_dict, coefficients=[0.25, 0.75])\n",
    "list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for coeff in [x / 20 for x in range(17, -1, -1)]:\n",
    "    wa = enrich_wa_states(list_states_dict, coefficients=[1 - coeff, coeff])\n",
    "    list_rewards_wa_samples = predict({\"wa\": wa}, samples_query_tensors)\n",
    "    print(coeff)\n",
    "    print(list_rewards_wa_samples)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
