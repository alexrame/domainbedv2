{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "import copy\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "\n",
    "class ScriptArguments:\n",
    "    #model_name = \"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment\"\n",
    "    sentiment_models = [\n",
    "        \"lvwerra/distilbert-imdb\",\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"martin-ha/toxic-comment-model\",\n",
    "        \"valurank/distilbert-quality\"\n",
    "    ]\n",
    "    model_names = [\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1.41e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-lr1e-05\",\n",
    "        # \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-distilbert-neg-lr1.41e-05\",\n",
    "        \"alexrame/gpt-neo-125M-imdb-lora-adapter-merged-ppo-sentiment-toxic-neg-lr1.41e-05\"\n",
    "    ]\n",
    "\n",
    "\n",
    "script_args = ScriptArguments()\n",
    "\n",
    "def load_model(peft_model_id):\n",
    "    peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        return_dict=True,\n",
    "        #torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # Load the Lora model\n",
    "    model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "dict_models_to_merge = OrderedDict({model_name: load_model(model_name) for model_name in script_args.model_names})\n",
    "# average\n",
    "def average_weights(input_models, coefficients):\n",
    "    \"\"\"average weights of different transformer models based on the amount of training data they were trained on\"\"\"\n",
    "    weights_averaged = OrderedDict()\n",
    "    for i, current_model in tqdm(enumerate(input_models), leave=False):\n",
    "        current_weights = current_model.state_dict()\n",
    "        for key in current_weights.keys():\n",
    "            if i == 0:\n",
    "                weights_averaged[key] = coefficients[i] * current_weights[key]\n",
    "            else:\n",
    "                weights_averaged[key] += coefficients[i] * current_weights[key]\n",
    "\n",
    "    return weights_averaged\n",
    "\n",
    "def enrich_wa(dict_models_to_merge):\n",
    "    weights_averaged = average_weights(dict_models_to_merge.values(), [1/len(dict_models_to_merge)]*len(dict_models_to_merge))\n",
    "    base_model_copy = list(dict_models_to_merge.values())[0]\n",
    "    base_model_copy.load_state_dict(weights_averaged, strict=True)\n",
    "    return base_model_copy\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PeftConfig.from_pretrained(script_args.model_names[0]).base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Load sentiment model with {script_args.sentiment_models}\")\n",
    "sentiment_pipes = [\n",
    "    pipeline(\"sentiment-analysis\", model=sentiment_model, device=device)\n",
    "    for sentiment_model in script_args.sentiment_models]\n",
    "\n",
    "def get_prediction_rewards(model, query_tensors):\n",
    "    def get_rewards(responses_text):\n",
    "        sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}\n",
    "        rewards = [\n",
    "            [sentiment_pipe(response_text, **sent_kwargs) for sentiment_pipe in sentiment_pipes]\n",
    "            for response_text in responses_text]\n",
    "        return rewards\n",
    "    def average_rewards(rewards):\n",
    "        avg_reward = None\n",
    "        for reward in rewards:\n",
    "            if avg_reward is None:\n",
    "                avg_reward = copy.deepcopy(reward)\n",
    "            else:\n",
    "                for a, r in zip(avg_reward, reward):\n",
    "                    for i, rr in enumerate(r):\n",
    "                        for j, rrr in enumerate(rr):\n",
    "                            assert a[i][j][\"label\"] == rrr[\"label\"]\n",
    "                            a[i][j][\"score\"] = a[i][j][\"score\"] + rrr[\"score\"]\n",
    "\n",
    "        for a in avg_reward:\n",
    "            for i, r in enumerate(a):\n",
    "                for j, rr in enumerate(r):\n",
    "                    rr[\"score\"] = rr[\"score\"] / len(rewards)\n",
    "        return avg_reward\n",
    "\n",
    "    response_tensors = []\n",
    "    responses_text = []\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    for i in range(len(query_tensors)):\n",
    "        query_tensor = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
    "        output = model.generate(input_ids=query_tensor, max_new_tokens=50).squeeze()\n",
    "        response_tensors.append(output)\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        responses_text.append(response)\n",
    "\n",
    "    rewards = get_rewards(responses_text)\n",
    "    reward = average_rewards(rewards)\n",
    "    return responses_text, rewards, reward\n",
    "\n",
    "\n",
    "def get_samples_query_tensors():\n",
    "    list_texts = [\n",
    "        \"I really enjoyed the slight hint towards\",\n",
    "        \"I really hated the horrible hint towards\"\n",
    "    ]\n",
    "\n",
    "    batch = tokenizer(list_texts, return_tensors=\"pt\")\n",
    "    return batch[\"input_ids\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(dict_models_to_merge, query_tensors):\n",
    "    for model_name, model in dict_models_to_merge.items():\n",
    "        responses_text, rewards, avg_reward = get_prediction_rewards(model, query_tensors)\n",
    "        print(\"model:\", model_name)\n",
    "        print(\"avg reward:\", avg_reward)\n",
    "        for text, reward in zip(responses_text, rewards):\n",
    "            print(\"text:\", text)\n",
    "            print(\"reward:\", reward)\n",
    "        print(\"\\n\")\n",
    "\n",
    "samples_query_tensors = get_samples_query_tensors()\n",
    "predict(dict_models_to_merge, samples_query_tensors)\n",
    "\n",
    "def get_imdb_query_tensors(bs=16):\n",
    "    ds = load_dataset(\"imdb\", split=\"test\")\n",
    "    ds = ds.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
    "\n",
    "    input_min_text_length=2\n",
    "    input_max_text_length=8\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    #### get a batch from the dataset\n",
    "    ds.set_format(\"pandas\")\n",
    "    df_batch = ds[:].sample(bs)\n",
    "    query_tensors = df_batch['input_ids'].tolist()\n",
    "    return query_tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(dict_models_to_merge, samples_query_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = enrich_wa(dict_models_to_merge)\n",
    "predict({\"wa\": wa}, samples_query_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict({\"wa\": wa}, imdb_query_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
