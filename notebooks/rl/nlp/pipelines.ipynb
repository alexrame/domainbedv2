{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1075], grad_fn=<SelectBackward0>) tensor([-0.6674], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = 'theblackcat102/deberta-v2-xxlarge-rm'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt = \"I just got out of prison, any suggestion?\"\n",
    "good_helpful = \"I am sorry to hear about it, it must be a hard time inside\"\n",
    "bad_text = \"Stay away from me, you scumbag convict\"\n",
    "pos = tokenizer(prompt, good_helpful, return_tensors='pt')\n",
    "neg = tokenizer(prompt, bad_text, return_tensors='pt')\n",
    "pos_score = model(**pos).logits[0]\n",
    "neg_score = model(**neg).logits[0]\n",
    "print(pos_score, neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/rame/trl/examples/llama\")\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3449], grad_fn=<SelectBackward0>) tensor([-2.0942], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'theblackcat102/reward-model-deberta-v3-base-v2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt = \"I just got out of prison, any suggestion?\"\n",
    "good_helpful = \"I am sorry to hear about it, it must be a hard time inside\"\n",
    "bad_text = \"Stay away from me, you scumbag convict\"\n",
    "pos = tokenizer(prompt, good_helpful, return_tensors='pt')\n",
    "neg = tokenizer(prompt, bad_text, return_tensors='pt')\n",
    "pos_score = model(**pos).logits[0]\n",
    "neg_score = model(**neg).logits[0]\n",
    "print(pos_score, neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  273,  348,  519,  321,  265, 3641,  261,  356, 7100,  302,    2,\n",
       "          273,  481, 3737,  264, 1275,  314,  278,  261,  278,  516,  282,  266,\n",
       "          657,  326, 1013,    2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,    16,    87,   260,    61,     9,  3863,     6,    95,  6769,\n",
       "            44,     2,  5474,   300,    34,    89,     6,    17, 90251, 39297,\n",
       "             2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] I just got out of prison, any suggestion?[SEP] I am sorry to hear about it, it must be a hard time inside[SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(pos[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "Loading from files only False\n"
     ]
    }
   ],
   "source": [
    "from llama_utils import Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentiment model: theblackcat102/reward-model-deberta-v3-base-v2\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipelines.load_pipe(model_name, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_assistant(sentiment_pipe, post, response):\n",
    "    if sentiment_pipe.model.name_or_path.startswith(\"OpenAssistant/\"):\n",
    "        return post + sentiment_pipe.tokenizer.sep_token + response\n",
    "    if sentiment_pipe.model.name_or_path.startswith(\"sugam11/gpt2-rlhf-reward\"):\n",
    "        return \"Human: \" + post + \" Assistant: \" + response\n",
    "    if sentiment_pipe.model.name_or_path.startswith(\"theblackcat102\"):\n",
    "        return post + sentiment_pipe.tokenizer.sep_token + response\n",
    "    if sentiment_pipe.model.name_or_path.startswith(\"ChaiML/\"):\n",
    "        return \"User: \" + post + \"\\nBot: \" + response\n",
    "    raise ValueError(sentiment_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [bad_text, good_helpful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I just got out of prison, any suggestion?[SEP]Stay away from me, you scumbag convict'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_text_assistant(sentiment_pipe=pipe, post=prompt, response=bad_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    transform_text_assistant(sentiment_pipe=pipe, post=prompt, response=response)\n",
    "    for response in responses\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 14 18:20:54 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A5000    On   | 00000000:AF:00.0 Off |                  Off |\r\n",
      "| 30%   29C    P8    19W / 230W |   1831MiB / 24564MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A   2411549      C   ...onda3/envs/nlp/bin/python     1828MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_outputs = pipe(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10966182990902203"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+math.exp(2.0942))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.10965858399868011},\n",
       " {'label': 'LABEL_0', 'score': 0.2067127376794815}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pair[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score[0][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_score[0][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/rame/trl/examples/llama/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name=llama_utils.Tokenizer.load_tokenizer_name(\"unitary/toxic-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"unitary/toxic-bert\",\n",
    "    device=device,\n",
    "    tokenizer=tokenizer_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Ban Ki-Moon was re-elected for a second term by the UN General Assembly, unopposed and unanimously, on 21 June 2011.\"\n",
    "\n",
    "bad_summary = \"Ban Ki-moon was elected for a second term in 2007.\"\n",
    "good_summary = \"Ban Ki-moon was elected for a second term in 2011.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Wow... what would you do in a situation like that!? The story of Breaking Point, though short, packed a lot of emotion. The acting was\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\n",
    "    text,\n",
    "    **{\n",
    "        \"return_all_scores\": True,\n",
    "        \"function_to_apply\": \"none\",\n",
    "        \"batch_size\": 1\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipes = [pipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_responses = [\n",
    "    (\"### Input:\" + article, bad_summary),\n",
    "    (\"### Input:\" + article, good_summary)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_utils.Instructions.get_input(\"### Input:\" + article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pair[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.tokenizer.decode(bad_pair[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.tokenizer.decode(pipe.tokenizer.encode(pairs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x, y) for x, y in zip(bad_pair[\"input_ids\"][0].tolist(), pipe.tokenizer.encode(pairs[0][0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\n",
    "        \n",
    "            llama_utils.transform_text_summary(\n",
    "                sentiment_pipe=sentiment_pipe,\n",
    "                post=llama_utils.Instructions.get_input(query),\n",
    "                response=response\n",
    "            )\n",
    "         for sentiment_pipe in sentiment_pipes\n",
    "    ] for query, response in queries_responses\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [\n",
    "    [\n",
    "        \n",
    "            sentiment_pipe(llama_utils.transform_text_summary(\n",
    "                sentiment_pipe=sentiment_pipe,\n",
    "                post=llama_utils.Instructions.get_input(query),\n",
    "                response=response,\n",
    "                \n",
    "            ),**{\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1})\n",
    "         for sentiment_pipe in sentiment_pipes\n",
    "    ] for query, response in queries_responses\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.tokenizer.encode(pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "                sentiment_pipe(\n",
    "                    llama_utils.transform_text_summary(\n",
    "                        sentiment_pipe=sentiment_pipe,\n",
    "                        post=llama_utils.Instructions.get_input(query),\n",
    "                        response=response\n",
    "                    ), **self.sent_kwargs\n",
    "                ) for sentiment_pipe in sentiment_pipes\n",
    "            ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpv2",
   "language": "python",
   "name": "nlpv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
