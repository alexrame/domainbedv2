{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/rame/trl/examples/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0][\"qid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrainf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"lvwerra/stack-exchange-paired\")\n",
    "\n",
    "dtrainf = ds[\"train\"].filter(lambda x: len(x[\"question\"]) < 300, batched=False)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def remove_duplicate(duplicated_dataset):\n",
    "    initial_list = duplicated_dataset.map(lambda x: {\"id\": x['qid']})\n",
    "    _ , unique_indices = np.unique(initial_list[\"qid\"], return_index=True, axis=0)\n",
    "    filtered_dataset = duplicated_dataset.select(unique_indices.tolist())\n",
    "    return filtered_dataset\n",
    "\n",
    "dtrainf_deduplicated = remove_duplicate(dtrainf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtestf = ds[\"test\"].filter(lambda x: len(x[\"question\"]) < 300, batched=False)\n",
    "dtestf_deduplicated = remove_duplicate(dtestf)\n",
    "dtestfs = dtestf_deduplicated.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"lvwerra/stack-exchange-paired\", split=\"train\", cache_dir=\"/data/rame/data/huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import llama_utils, ppo_utils, args_utils\n",
    "# see this https://github.com/lvwerra/trl/blob/main/examples/stack_llama/scripts/rl_training.py\n",
    "MIN_SIZE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = {\"train\": \"train\", \"validation\": \"test\"}[split]\n",
    "if args_utils.LOCAL_FILES_ONLY:\n",
    "    ds = load_from_disk(\"/gpfsdswork/projects/rech/edr/utr15kn/dataplace/data/huggingface/stack_data_sampled\")\n",
    "    ds = ds[split]\n",
    "    # if split == \"train\":\n",
    "    #     ds = ds.select(range(20000))\n",
    "    # else:\n",
    "    #     ds = ds.select(range(1000))\n",
    "else:\n",
    "    ds = load_from_disk(\"/data/rame/data/huggingface/stack_data_sampled\")\n",
    "    # ds = load_dataset(\n",
    "    #     \"lvwerra/stack-exchange-paired\",\n",
    "    #     cache_dir=\"/data/rame/data/huggingface\", split=split)\n",
    "    # ds = ds.select(range(20000))\n",
    "    ds = ds[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"lvwerra/stack-exchange-paired\", split=\"train\", cache_dir)\n",
    "ds = ds.select(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Anthropic/hh-rlhf\", name=\"comparisons\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered = ds.filter(\n",
    "    lambda x: x[\"chosen\"] is not None and MIN_SIZE <\n",
    "    len(x[\"chosen\"].split(\"Assistant: \")[0]) < 80,\n",
    "    batched=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_sampler = LengthSampler(2, 8)\n",
    "\n",
    "def tokenize(sample):\n",
    "    text = sample[\"chosen\"].replace(\"\\n\", \" \")\n",
    "    instruction = text.split(\"Assistant: \")[0].split(\"Human: \")[1]\n",
    "    prompt = llama_utils.Instructions.get_prompt_noinput(instruction=instruction,)\n",
    "    response = text.split(\"Assistant: \")[1]\n",
    "    size_prompt = len(tokenizer.encode(prompt)) - 1\n",
    "    input_size = size_prompt + input_size_sampler()\n",
    "    sample[\"input_ids\"] = tokenizer.encode(prompt + response)[:input_size]\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "ds_mapped = ds_filtered.map(tokenize, batched=False, load_from_cache_file=False)\n",
    "ds_mapped.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mapped[\"query\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.filter(\n",
    "    lambda x: x[\"chosen\"] is not None and MIN_SIZE < len(x) < max_size and x[\n",
    "        'info'][\"id\"] is not None,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import llama_utils, ppo_utils, args_utils\n",
    "\n",
    "MIN_SIZE = 100\n",
    "\n",
    "\n",
    "def build_dataset(dataset_name, *args, **kwargs):\n",
    "    if dataset_name == \"news\":\n",
    "        return _build_news_dataset(*args, **kwargs)\n",
    "    else:\n",
    "        return _build_openai_dataset(*args, **kwargs)\n",
    "\n",
    "\n",
    "def _build_news_dataset(tokenizer, split=\"train\", max_size=1500):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_name (`str`): \"argilla/news-summary\"\n",
    "    \"\"\"\n",
    "    split = {\"train\": \"test\", \"validation\": \"train\"}[split]\n",
    "    ds = load_dataset(\"argilla/news-summary\", name=\"comparisons\", split=split, use_auth_token=True)\n",
    "    ds_filtered = ds.filter(\n",
    "        lambda x: x[\"text\"] is not None and MIN_SIZE < len(x[\"text\"]) < max_size and x[\"id\"] is\n",
    "        not None,\n",
    "        batched=False\n",
    "    )\n",
    "\n",
    "    def remove_duplicate(duplicated_dataset):\n",
    "        initial_list = duplicated_dataset.map(lambda x: {\"id\": x['id']})\n",
    "        _, unique_indices = np.unique(initial_list[\"id\"], return_index=True, axis=0)\n",
    "        filtered_dataset = duplicated_dataset.select(unique_indices.tolist())\n",
    "        return filtered_dataset\n",
    "\n",
    "    ds_deduplicated = remove_duplicate(ds_filtered)\n",
    "    input_size_sampler = LengthSampler(2, 8)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        info_post = \"-\".join(sample[\"text\"].replace(\"\\n\", \" \").split(\"(Reuters) -\")[1:]).strip()\n",
    "        prompt_summary = llama_utils.Instructions.get_prompt_summary(post=info_post)\n",
    "        size_prompt_summary = len(tokenizer.encode(prompt_summary)) - 1\n",
    "        input_size = size_prompt_summary + input_size_sampler()\n",
    "        choice = 0  # select the best summary\n",
    "        response = sample[\"prediction\"][choice][\"text\"].replace(\"\\n\", \" \").replace(\".\", \",\")\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt_summary + response)[:input_size]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds_mapped = ds_deduplicated.map(tokenize, batched=False, load_from_cache_file=False)\n",
    "    ds_mapped.set_format(type=\"torch\")\n",
    "    return ds_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llama_utils.Tokenizer.load_tokenizer(\"decapoda-research/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(tokenizer=tokenizer, dataset_name=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "# df_batch = ds[:].sample(bs)\n",
    "df_batch = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch[\"query\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d[\"query\"] for d in dataset[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/summarize_from_feedback\", name=\"comparisons\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf = ds.filter(lambda x: len(x[\"info\"][\"post\"]) < 1200, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\na \\n aljkka \\n\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"summaries\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CogComp/bart-faithful-summary-detector\")\n",
    "\n",
    "article = \"Ban Ki-Moon was re-elected for a second term by the UN General Assembly, unopposed and unanimously, on 21 June 2011.\"\n",
    "\n",
    "bad_summary = \"Ban Ki-moon was elected for a second term in 2007.\"\n",
    "good_summary = \"Ban Ki-moon was elected for a second term in 2011.\"\n",
    "\n",
    "bad_pair = tokenizer(text=bad_summary, text_pair=article, return_tensors='pt')\n",
    "good_pair = tokenizer(text=good_summary, text_pair=article, return_tensors='pt')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"CogComp/bart-faithful-summary-detector\")\n",
    "\n",
    "bad_score = model(**bad_pair)\n",
    "good_score = model(**good_pair)\n",
    "good_score[0][:, 1]\n",
    "bad_score[0][:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/rame/trl/examples/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = llama_utils.Pipelines.load_pipe(sentiment_model=\"Tristan/gpt2_reward_summarization\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe_v2 = llama_utils.Pipelines.load_pipe(sentiment_model=\"CogComp/bart-faithful-summary-detector\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Ban Ki-moon was a very good president.\",\n",
    "    \"Ban Ki-moon was elected for a second term in 2011.\",\n",
    "    \"Zinedine Yazid Zidane, popularly known as Zizou, is a French professional football manager and former player who played as an attacking midfielder.\"\n",
    "    \n",
    "]\n",
    "article = \"Ban Ki-Moon was re-elected for a second term by the UN General Assembly, unopposed and unanimously, on 21 June 2011.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_v1 = [\n",
    "    llama_utils.transform_text(\n",
    "        sentiment_pipe=sentiment_pipe,\n",
    "        response_text=text,\n",
    "        instruction=article\n",
    "    ) for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_v2 = [\n",
    "    llama_utils.transform_text(\n",
    "        sentiment_pipe=sentiment_pipe,\n",
    "        response_text=text,\n",
    "        instruction=article\n",
    "    ) for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe(texts_v1, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe_v2(texts_v2, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Tristan/gpt2_reward_summarization\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Tristan/gpt2_reward_summarization\")\n",
    "\n",
    "\n",
    "bad_summary = \"Ban Ki-moon was a very good president.\"\n",
    "good_summary = \n",
    "\n",
    "\n",
    "bad_input_ids = tokenizer.encode(\"summary:\"+ bad_summary + tokenizer.eos_token + \"article:\" + article)\n",
    "bad_score = model(input_ids=torch.tensor([bad_input_ids]))[0]\n",
    "print(\"bad\", bad_score[0].detach())\n",
    "good_input_ids = tokenizer.encode(\"summary:\" + good_summary + tokenizer.eos_token + \"article:\" + article)\n",
    "good_score = model(input_ids=torch.tensor([good_input_ids]))[0]\n",
    "print(\"good\", good_score[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_summary = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_text = good_summary + \" \" + tokenizer.bos_token + \" \" + article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_input_ids = tokenizer.encode(bad_summary + \" \" + tokenizer.bos_token + \" \" + article)\n",
    "bad_score = model(input_ids=torch.tensor([bad_input_ids]))[0]\n",
    "print(\"bad\", bad_score[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_input_ids = tokenizer.encode(response_text)\n",
    "good_score = model(input_ids=torch.tensor([good_input_ids]))[0]\n",
    "print(\"good\", good_score[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_into_text_classification_format(examples):\n",
    "    new_examples = {\"text_j\": [], \"text_k\": []}\n",
    "    for info, summaries, choice in zip(examples[\"info\"], examples[\"summaries\"], examples[\"choice\"]):\n",
    "        if len(summaries) != 2 or choice not in (0, 1):\n",
    "            raise ValueError(\n",
    "                f\"There should be two summaries with a choice that's either 0 or 1. Received {len(summaries)} summaries and choice={choice}.\"\n",
    "            )\n",
    "        original_text_field = \"post\" if info[\"post\"] is not None else \"article\"\n",
    "        new_examples[\"text_j\"].append(\n",
    "            summaries[choice][\"text\"] + \" \" + tokenizer.bos_token + \" \" + info[original_text_field]\n",
    "        )\n",
    "        new_examples[\"text_k\"].append(\n",
    "            summaries[0 if choice == 1 else 1][\"text\"] + \" \" + tokenizer.bos_token + \" \" + info[original_text_field]\n",
    "        )\n",
    "\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
