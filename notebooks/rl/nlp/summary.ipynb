{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/rame/trl/examples/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "libcurand.so.10: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rame/anaconda3/envs/nlp/lib/python3.8/site-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import llama_utils, ppo_utils, args_utils\n",
    "\n",
    "MIN_SIZE = 100\n",
    "\n",
    "\n",
    "def build_dataset(dataset_name, *args, **kwargs):\n",
    "    if dataset_name == \"news\":\n",
    "        return _build_news_dataset(*args, **kwargs)\n",
    "    else:\n",
    "        return _build_openai_dataset(*args, **kwargs)\n",
    "\n",
    "\n",
    "def _build_news_dataset(tokenizer, split=\"train\", max_size=1500):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_name (`str`): \"argilla/news-summary\"\n",
    "    \"\"\"\n",
    "    split = {\"train\": \"test\", \"validation\": \"train\"}[split]\n",
    "    ds = load_dataset(\"argilla/news-summary\", name=\"comparisons\", split=split, use_auth_token=True)\n",
    "    ds_filtered = ds.filter(\n",
    "        lambda x: x[\"text\"] is not None and MIN_SIZE < len(x[\"text\"]) < max_size and x[\"id\"] is\n",
    "        not None,\n",
    "        batched=False\n",
    "    )\n",
    "\n",
    "    def remove_duplicate(duplicated_dataset):\n",
    "        initial_list = duplicated_dataset.map(lambda x: {\"id\": x['id']})\n",
    "        _, unique_indices = np.unique(initial_list[\"id\"], return_index=True, axis=0)\n",
    "        filtered_dataset = duplicated_dataset.select(unique_indices.tolist())\n",
    "        return filtered_dataset\n",
    "\n",
    "    ds_deduplicated = remove_duplicate(ds_filtered)\n",
    "    input_size_sampler = LengthSampler(2, 8)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        info_post = \"-\".join(sample[\"text\"].replace(\"\\n\", \" \").split(\"(Reuters) -\")[1:]).strip()\n",
    "        prompt_summary = llama_utils.Instructions.get_prompt_summary(post=info_post)\n",
    "        size_prompt_summary = len(tokenizer.encode(prompt_summary)) - 1\n",
    "        input_size = size_prompt_summary + input_size_sampler()\n",
    "        choice = 0  # select the best summary\n",
    "        response = sample[\"prediction\"][choice][\"text\"].replace(\"\\n\", \" \").replace(\".\", \",\")\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt_summary + response)[:input_size]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds_mapped = ds_deduplicated.map(tokenize, batched=False, load_from_cache_file=False)\n",
    "    ds_mapped.set_format(type=\"torch\")\n",
    "    return ds_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = llama_utils.Tokenizer.load_tokenizer(\"decapoda-research/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/rame/.cache/huggingface/datasets/argilla___parquet/argilla--news-summary-46ccad7a40bceec1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/rame/.cache/huggingface/datasets/argilla___parquet/argilla--news-summary-46ccad7a40bceec1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4641da6c18afa7bf.arrow\n",
      "Loading cached processed dataset at /home/rame/.cache/huggingface/datasets/argilla___parquet/argilla--news-summary-46ccad7a40bceec1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8a4223dbadb9a091.arrow\n",
      "                                                                                                                                                                                                                                                            \r"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(tokenizer=tokenizer, dataset_name=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'id', 'metadata', 'status', 'event_timestamp', 'metrics', 'input_ids', 'query'],\n",
      "    num_rows: 6971\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "# df_batch = ds[:].sample(bs)\n",
    "df_batch = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch[\"query\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d[\"query\"] for d in dataset[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/summarize_from_feedback\", name=\"comparisons\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf = ds.filter(lambda x: len(x[\"info\"][\"post\"]) < 1200, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\na \\n aljkka \\n\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"summaries\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CogComp/bart-faithful-summary-detector\")\n",
    "\n",
    "article = \"Ban Ki-Moon was re-elected for a second term by the UN General Assembly, unopposed and unanimously, on 21 June 2011.\"\n",
    "\n",
    "bad_summary = \"Ban Ki-moon was elected for a second term in 2007.\"\n",
    "good_summary = \"Ban Ki-moon was elected for a second term in 2011.\"\n",
    "\n",
    "bad_pair = tokenizer(text=bad_summary, text_pair=article, return_tensors='pt')\n",
    "good_pair = tokenizer(text=good_summary, text_pair=article, return_tensors='pt')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"CogComp/bart-faithful-summary-detector\")\n",
    "\n",
    "bad_score = model(**bad_pair)\n",
    "good_score = model(**good_pair)\n",
    "good_score[0][:, 1]\n",
    "bad_score[0][:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/rame/trl/examples/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = llama_utils.Pipelines.load_pipe(sentiment_model=\"Tristan/gpt2_reward_summarization\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe_v2 = llama_utils.Pipelines.load_pipe(sentiment_model=\"CogComp/bart-faithful-summary-detector\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Ban Ki-moon was a very good president.\",\n",
    "    \"Ban Ki-moon was elected for a second term in 2011.\",\n",
    "    \"Zinedine Yazid Zidane, popularly known as Zizou, is a French professional football manager and former player who played as an attacking midfielder.\"\n",
    "    \n",
    "]\n",
    "article = \"Ban Ki-Moon was re-elected for a second term by the UN General Assembly, unopposed and unanimously, on 21 June 2011.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_v1 = [\n",
    "    llama_utils.transform_text(\n",
    "        sentiment_pipe=sentiment_pipe,\n",
    "        response_text=text,\n",
    "        instruction=article\n",
    "    ) for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_v2 = [\n",
    "    llama_utils.transform_text(\n",
    "        sentiment_pipe=sentiment_pipe,\n",
    "        response_text=text,\n",
    "        instruction=article\n",
    "    ) for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe(texts_v1, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe_v2(texts_v2, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Tristan/gpt2_reward_summarization\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Tristan/gpt2_reward_summarization\")\n",
    "\n",
    "\n",
    "bad_summary = \"Ban Ki-moon was a very good president.\"\n",
    "good_summary = \n",
    "\n",
    "\n",
    "bad_input_ids = tokenizer.encode(\"summary:\"+ bad_summary + tokenizer.eos_token + \"article:\" + article)\n",
    "bad_score = model(input_ids=torch.tensor([bad_input_ids]))[0]\n",
    "print(\"bad\", bad_score[0].detach())\n",
    "good_input_ids = tokenizer.encode(\"summary:\" + good_summary + tokenizer.eos_token + \"article:\" + article)\n",
    "good_score = model(input_ids=torch.tensor([good_input_ids]))[0]\n",
    "print(\"good\", good_score[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_summary = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_text = good_summary + \" \" + tokenizer.bos_token + \" \" + article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_input_ids = tokenizer.encode(bad_summary + \" \" + tokenizer.bos_token + \" \" + article)\n",
    "bad_score = model(input_ids=torch.tensor([bad_input_ids]))[0]\n",
    "print(\"bad\", bad_score[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_input_ids = tokenizer.encode(response_text)\n",
    "good_score = model(input_ids=torch.tensor([good_input_ids]))[0]\n",
    "print(\"good\", good_score[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_into_text_classification_format(examples):\n",
    "    new_examples = {\"text_j\": [], \"text_k\": []}\n",
    "    for info, summaries, choice in zip(examples[\"info\"], examples[\"summaries\"], examples[\"choice\"]):\n",
    "        if len(summaries) != 2 or choice not in (0, 1):\n",
    "            raise ValueError(\n",
    "                f\"There should be two summaries with a choice that's either 0 or 1. Received {len(summaries)} summaries and choice={choice}.\"\n",
    "            )\n",
    "        original_text_field = \"post\" if info[\"post\"] is not None else \"article\"\n",
    "        new_examples[\"text_j\"].append(\n",
    "            summaries[choice][\"text\"] + \" \" + tokenizer.bos_token + \" \" + info[original_text_field]\n",
    "        )\n",
    "        new_examples[\"text_k\"].append(\n",
    "            summaries[0 if choice == 1 else 1][\"text\"] + \" \" + tokenizer.bos_token + \" \" + info[original_text_field]\n",
    "        )\n",
    "\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpv2",
   "language": "python",
   "name": "nlpv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
